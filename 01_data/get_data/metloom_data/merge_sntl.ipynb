{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/dlhogan/GitHub/Synoptic-Sublimation/01_data/raw_data/station_data/long_term_data/sntl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sntl(path):\n",
    "    for file in glob.glob(path+'/*.txt'):\n",
    "        print('Working on file: ', file.split('/')[-1])\n",
    "        df = pd.read_csv(file, header=77)\n",
    "        # set Date as index\n",
    "        df = df.set_index('Date')\n",
    "        # first lets separate the sites and the site_ids\n",
    "        sites = sorted(set([int(d.split('(')[0].split(')')[0]) for d in df.columns.to_list()]), key=int)\n",
    "        site_ids = sorted(set([int(d.split('(')[1].split(')')[0]) for d in df.columns.to_list()]), key=int)\n",
    "\n",
    "        # now get the columns that contain each site\n",
    "        site_columns = {}\n",
    "\n",
    "        for i,site in enumerate(sites):\n",
    "            site_columns[site] = {'original':[col for col in df.columns.to_list() if site in col],\n",
    "                                    'cleaned':[col.split(') ')[1] for col in df.columns.to_list() if site in col]}\n",
    "            \n",
    "            tmp_df = df[site_columns[site]['original']].rename(dict(zip(site_columns[site]['original'],\n",
    "                                                                        site_columns[site]['cleaned'])), axis=1)\n",
    "            tmp_df.index = pd.to_datetime(tmp_df.index)\n",
    "            # map QC flag columns to their respective data columns\n",
    "            data_columns = [col for col in tmp_df.columns.to_list() if 'QC' not in col]\n",
    "            qc_columns = [col for col in tmp_df.columns.to_list() if 'QC' in col]\n",
    "            qc_map = {data_columns[i]:col for i,col in enumerate(qc_columns)}\n",
    "            # convert to a tidy format\n",
    "            tidy_tmp_df = tmp_df.melt(ignore_index=False, var_name='variable', value_name='value')\n",
    "            # create a flag column with the values for\n",
    "            # # create a new column called flag\n",
    "            # tidy_tmp_df['qc_flag'] = ''\n",
    "            # for col in data_columns:\n",
    "            #     tidy_tmp_df.loc[tidy_tmp_df['variable']==col,'qc_flag'] = tidy_tmp_df[tidy_tmp_df['variable']==qc_map[col]]['value']\n",
    "            # remove the rows where the variable is a QC flag\n",
    "            tidy_tmp_df = tidy_tmp_df[tidy_tmp_df['variable'].isin(data_columns)]\n",
    "            # explode the dataframe\n",
    "            tidy_tmp_df = tidy_tmp_df.explode('value')\n",
    "            # save to a csv file with the site name and site id\n",
    "            tidy_tmp_df.to_csv(f'../../raw_data/station_data/long_term_data/sntl_tidy/{site}_{site_ids[i]}_SNTL_tidy.csv')\n",
    "            print(f'Saved {site} to {site}_{site_ids[i]}_SNTL_tidy.csv')\n",
    "        return tidy_tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Butte ',\n",
       " 'Cascade ',\n",
       " 'Cascade #2 ',\n",
       " 'El Diente Peak ',\n",
       " 'Hoosier Pass ',\n",
       " 'Idarado ',\n",
       " 'Lizard Head Pass ',\n",
       " 'Mineral Creek ',\n",
       " 'Molas Lake ',\n",
       " 'Park Cone ',\n",
       " 'Red Mountain Pass ',\n",
       " 'Schofield Pass ',\n",
       " 'Scotch Creek ',\n",
       " 'Spud Mountain ',\n",
       " 'Upper Taylor '}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([(d.split('(')[0].split(')')[0]) for d in df.columns.to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on file:  all_sntls.txt\n",
      "Saved Butte to Butte_380_SNTL_tidy.csv\n",
      "Saved Cascade to Cascade_386_SNTL_tidy.csv\n",
      "Saved Cascade #2 to Cascade #2_387_SNTL_tidy.csv\n",
      "Saved El Diente Peak to El Diente Peak_465_SNTL_tidy.csv\n",
      "Saved Hoosier Pass to Hoosier Pass_531_SNTL_tidy.csv\n",
      "Saved Idarado to Idarado_538_SNTL_tidy.csv\n",
      "Saved Lizard Head Pass to Lizard Head Pass_586_SNTL_tidy.csv\n",
      "Saved Mineral Creek to Mineral Creek_629_SNTL_tidy.csv\n",
      "Saved Molas Lake to Molas Lake_632_SNTL_tidy.csv\n",
      "Saved Park Cone to Park Cone_680_SNTL_tidy.csv\n",
      "Saved Red Mountain Pass to Red Mountain Pass_713_SNTL_tidy.csv\n",
      "Saved Schofield Pass to Schofield Pass_737_SNTL_tidy.csv\n",
      "Saved Scotch Creek to Scotch Creek_739_SNTL_tidy.csv\n",
      "Saved Spud Mountain to Spud Mountain_780_SNTL_tidy.csv\n",
      "Saved Upper Taylor to Upper Taylor_1141_SNTL_tidy.csv\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(path+'/*.txt'):\n",
    "        print('Working on file: ', file.split('/')[-1])\n",
    "        df = pd.read_csv(file, header=77)\n",
    "        # set Date as index\n",
    "        df = df.set_index('Date')\n",
    "        # first lets separate the sites and the site_ids\n",
    "        sites = list(sorted(set([(d.split('(')[0].split(')')[0][:-1]) for d in df.columns.to_list()]), key=str))\n",
    "        site_ids = list(sorted(set([int(d.split('(')[1].split(')')[0]) for d in df.columns.to_list()]), key=int))\n",
    "\n",
    "        # now get the columns that contain each site\n",
    "        site_columns = {}\n",
    "\n",
    "        for i,site in enumerate(sites):\n",
    "            site_columns[site] = {'original':[col for col in df.columns.to_list() if site in col],\n",
    "                                    'cleaned':[col.split(') ')[1] for col in df.columns.to_list() if site in col]}\n",
    "            \n",
    "            tmp_df = df[site_columns[site]['original']].rename(dict(zip(site_columns[site]['original'],\n",
    "                                                                        site_columns[site]['cleaned'])), axis=1)\n",
    "            tmp_df.index = pd.to_datetime(tmp_df.index)\n",
    "            # map QC flag columns to their respective data columns\n",
    "            data_columns = [col for col in tmp_df.columns.to_list() if 'QC' not in col]\n",
    "            qc_columns = [col for col in tmp_df.columns.to_list() if 'QC' in col]\n",
    "            qc_map = {data_columns[i]:col for i,col in enumerate(qc_columns)}\n",
    "            # convert to a tidy format\n",
    "            tidy_tmp_df = tmp_df.melt(ignore_index=False, var_name='variable', value_name='value')\n",
    "            # create a flag column with the values for\n",
    "            # # create a new column called flag\n",
    "            # tidy_tmp_df['qc_flag'] = ''\n",
    "            # for col in data_columns:\n",
    "            #     tidy_tmp_df.loc[tidy_tmp_df['variable']==col,'qc_flag'] = tidy_tmp_df[tidy_tmp_df['variable']==qc_map[col]]['value']\n",
    "            # remove the rows where the variable is a QC flag\n",
    "            tidy_tmp_df = tidy_tmp_df[tidy_tmp_df['variable'].isin(data_columns)]\n",
    "            # explode the dataframe\n",
    "            # tidy_tmp_df = tidy_tmp_df.explode('value')\n",
    "            # save to a csv file with the site name and site id\n",
    "            tidy_tmp_df.to_csv(f'../../raw_data/station_data/long_term_data/sntl_tidy/{site}_{site_ids[i]}_SNTL_tidy.csv')\n",
    "            print(f'Saved {site} to {site}_{site_ids[i]}_SNTL_tidy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on file:  all_sntls.txt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tidy_tmp_df \u001b[38;5;241m=\u001b[39m \u001b[43mclean_sntl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 15\u001b[0m, in \u001b[0;36mclean_sntl\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     12\u001b[0m site_columns \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,site \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sites):\n\u001b[0;32m---> 15\u001b[0m     site_columns[site] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     16\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned\u001b[39m\u001b[38;5;124m'\u001b[39m:[col\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_list() \u001b[38;5;28;01mif\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m col]}\n\u001b[1;32m     18\u001b[0m     tmp_df \u001b[38;5;241m=\u001b[39m df[site_columns[site][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(site_columns[site][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m                                                                 site_columns[site][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned\u001b[39m\u001b[38;5;124m'\u001b[39m])), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     tmp_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(tmp_df\u001b[38;5;241m.\u001b[39mindex)\n",
      "Cell \u001b[0;32mIn[42], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m site_columns \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,site \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sites):\n\u001b[0;32m---> 15\u001b[0m     site_columns[site] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m:[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_list() \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m],\n\u001b[1;32m     16\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned\u001b[39m\u001b[38;5;124m'\u001b[39m:[col\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_list() \u001b[38;5;28;01mif\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m col]}\n\u001b[1;32m     18\u001b[0m     tmp_df \u001b[38;5;241m=\u001b[39m df[site_columns[site][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(site_columns[site][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m                                                                 site_columns[site][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned\u001b[39m\u001b[38;5;124m'\u001b[39m])), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     tmp_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(tmp_df\u001b[38;5;241m.\u001b[39mindex)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not int"
     ]
    }
   ],
   "source": [
    "tidy_tmp_df = clean_sntl(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metloom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
