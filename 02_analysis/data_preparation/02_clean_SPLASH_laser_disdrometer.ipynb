{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import glob as glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean SPLASH Laser Disdrometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already downloaded\n"
     ]
    }
   ],
   "source": [
    "# set the filepath\n",
    "filepath = '/storage/dlhogan/synoptic_sublimation/splash_data/'\n",
    "if not os.path.exists(filepath+'NOAA_PSL_OttDisdrometerStats_KettlePonds'):\n",
    "    print('Data not downlaoded yet. Would you like to download it?')\n",
    "    download = input('y/n: ')\n",
    "    if download == 'y':\n",
    "        # downlaod the data from https://zenodo.org/records/10368926/files/NOAA_PSL_OttDisdrometerRaw_KettlePonds.zip using wget to the filepath\n",
    "        os.system('wget https://zenodo.org/records/10368926/files/NOAA_PSL_OttDisdrometerRaw_KettlePonds.zip -P '+filepath)\n",
    "        # unzip the file\n",
    "        os.system('unzip '+filepath+'NOAA_PSL_OttDisdrometerRaw_KettlePonds.zip -d '+filepath)\n",
    "        # remove the zip file\n",
    "        os.system('rm '+filepath+'NOAA_PSL_OttDisdrometerRaw_KettlePonds.zip')\n",
    "    else:\n",
    "        print('Download the data from https://zenodo.org/records/10368926/files/NOAA_PSL_OttDisdrometerRaw_KettlePonds.zip')\n",
    "else:\n",
    "    print('Data already downloaded')\n",
    "    # we'll start by loading in one file and looking at the data\n",
    "    filepath = '/storage/dlhogan/synoptic_sublimation/splash_data/NOAA_PSL_OttDisdrometerStats_KettlePonds/*'\n",
    "    files = glob.glob(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Generated with the help of ChatGPT 4.0 at OpenAI. Link to prompts: https://chatgpt.com/share/52f25a1e-c008-43f0-80c2-7d44dde02cd7\n",
    "\"\"\"\n",
    "\n",
    "def process_laser_disdrometer_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a laser disdrometer file and return a xarray Dataset with the data.\n",
    "    \"\"\"\n",
    "    # Define the size bins\n",
    "    size_bins = [\n",
    "        0.062, 0.187, 0.312, 0.437, 0.562, 0.687, 0.812, 0.937, 1.062, 1.187,\n",
    "        1.375, 1.625, 1.875, 2.125, 2.375, 2.75, 3.25, 3.75, 4.25, 4.75,\n",
    "        5.5, 6.5, 7.5, 8.5, 9.5, 11.0, 13.0, 15.0, 17.0, 19.0, 21.5, 24.5\n",
    "    ]\n",
    "\n",
    "    # Read the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Parse the header for metadata (assuming first line is header)\n",
    "    header = lines[0].strip()\n",
    "    instrument_info, time_info = header.split(\" Time (YYJJJHH): \")\n",
    "    year = int(\"20\" + time_info[:2])\n",
    "    day_of_year = int(time_info[2:5])\n",
    "    start_hour = int(time_info[5:7])\n",
    "\n",
    "    # Skip the first two lines\n",
    "    lines = lines[2:]\n",
    "    \n",
    "    # Initialize a list to store parsed data\n",
    "    data = []\n",
    "\n",
    "    # Process each line in the file\n",
    "    for i, line in enumerate(lines):\n",
    "        fields = line.strip().split()\n",
    "        \n",
    "        # Skip lines that don't have the correct number of fields\n",
    "        if len(fields) != 57:\n",
    "            continue\n",
    "        \n",
    "        # Extract and convert data fields\n",
    "        begin_time_str = fields[0].split('-')[0]\n",
    "        end_time_str = fields[0].split('-')[1]\n",
    "        particle_distribution = list(map(int, fields[1:33]))\n",
    "        qc_data = list(map(int, fields[33:36]))\n",
    "        precip_stats = list(map(float, fields[36:41]))\n",
    "        laser_status = list(map(float, fields[41:47]))\n",
    "        sensor_status = list(map(float, fields[47:53]))\n",
    "        precip_partitioning = list(map(int, fields[53:57]))\n",
    "\n",
    "        # Convert begin_time and end_time to timestamps\n",
    "        begin_time = pd.Timestamp(year, 1, 1) + pd.Timedelta(days=day_of_year-1, hours=start_hour,\n",
    "                                                                minutes=int(begin_time_str[0:2]), \n",
    "                                                                seconds=int(begin_time_str[3:5]),\n",
    "                                                                milliseconds=int(begin_time_str[6:9]))\n",
    "        end_time = pd.Timestamp(year, 1, 1) + pd.Timedelta(days=day_of_year-1, hours=start_hour,\n",
    "                                                                minutes=int(end_time_str[0:2]), \n",
    "                                                                seconds=int(end_time_str[3:5]), \n",
    "                                                                milliseconds=int(end_time_str[6:9]))\n",
    "        # If end time is before begin time, increment the hour by 1\n",
    "        if end_time < begin_time:\n",
    "            end_time += pd.Timedelta(hours=1)\n",
    "        \n",
    "        data.append({\n",
    "            \"time\": begin_time,\n",
    "            \"particle_distribution\": particle_distribution,\n",
    "            \"qc_data\": qc_data,\n",
    "            \"precip_stats\": precip_stats,\n",
    "            \"laser_status\": laser_status,\n",
    "            \"sensor_status\": sensor_status,\n",
    "            \"precip_partitioning\": precip_partitioning\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Ensure all lists in df have consistent lengths\n",
    "    assert all(len(item) == 32 for item in df[\"particle_distribution\"]), \"Mismatch in particle_distribution length\"\n",
    "    assert all(len(item) == 3 for item in df[\"qc_data\"]), \"Mismatch in qc_data length\"\n",
    "    assert all(len(item) == 5 for item in df[\"precip_stats\"]), \"Mismatch in precip_stats length\"\n",
    "    assert all(len(item) == 6 for item in df[\"laser_status\"]), \"Mismatch in laser_status length\"\n",
    "    assert all(len(item) == 6 for item in df[\"sensor_status\"]), \"Mismatch in sensor_status length\"\n",
    "    assert all(len(item) == 4 for item in df[\"precip_partitioning\"]), \"Mismatch in precip_partitioning length\"\n",
    "\n",
    "    # Convert DataFrame to xarray Dataset\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"particle_distribution\": ((\"time\", \"size_bins\"), np.stack(df[\"particle_distribution\"].values)),\n",
    "            \"Blackout\": (\"time\", df[\"qc_data\"].apply(lambda x: x[0])),\n",
    "            \"Good\": (\"time\", df[\"qc_data\"].apply(lambda x: x[1])),\n",
    "            \"Bad\": (\"time\", df[\"qc_data\"].apply(lambda x: x[2])),\n",
    "            \"NumParticle\": (\"time\", df[\"precip_stats\"].apply(lambda x: x[0])),\n",
    "            \"Rate\": (\"time\", df[\"precip_stats\"].apply(lambda x: x[1]), {\"units\": \"mm/h\", \"descriptor\": \"Precipitation\"}),\n",
    "            \"Amount\": (\"time\", df[\"precip_stats\"].apply(lambda x: x[2]), {\"units\": \"mm\", \"descriptor\": \"Precipitation\"}),\n",
    "            \"AmountSum\": (\"time\", df[\"precip_stats\"].apply(lambda x: x[3]), {\"units\": \"mm\", \"descriptor\": \"Precipitation\"}),\n",
    "            \"Z\": (\"time\", df[\"precip_stats\"].apply(lambda x: x[4]), {\"units\": \"dB\", \"descriptor\": \"Precipitation\"}),\n",
    "            \"NumError\": (\"time\", df[\"laser_status\"].apply(lambda x: x[0])),\n",
    "            \"Dirty\": (\"time\", df[\"laser_status\"].apply(lambda x: x[1])),\n",
    "            \"VeryDirty\": (\"time\", df[\"laser_status\"].apply(lambda x: x[2])),\n",
    "            \"Damaged\": (\"time\", df[\"laser_status\"].apply(lambda x: x[3])),\n",
    "            \"SignalAvg\": (\"time\", df[\"laser_status\"].apply(lambda x: x[4])),\n",
    "            \"SignalStdDev\": (\"time\", df[\"laser_status\"].apply(lambda x: x[5])),\n",
    "            \"TempAvg\": (\"time\", df[\"sensor_status\"].apply(lambda x: x[0]), {\"units\": \"C\", \"descriptor\": \"Sensor Status\"}),\n",
    "            \"TempStdDev\": (\"time\", df[\"sensor_status\"].apply(lambda x: x[1]), {\"units\": \"C\", \"descriptor\": \"Sensor Status\"}),\n",
    "            \"VoltAvg\": (\"time\", df[\"sensor_status\"].apply(lambda x: x[2]), {\"units\": \"V\", \"descriptor\": \"Sensor Status\"}),\n",
    "            \"VoltStdDev\": (\"time\", df[\"sensor_status\"].apply(lambda x: x[3]), {\"units\": \"V\", \"descriptor\": \"Sensor Status\"}),\n",
    "            \"HeatCurrentAvg\": (\"time\", df[\"sensor_status\"].apply(lambda x: x[4]), {\"units\": \"A\", \"descriptor\": \"Sensor Status\"}),\n",
    "            \"HeatCurrentStdDev\": (\"time\", df[\"sensor_status\"].apply(lambda x: x[5]), {\"units\": \"A\", \"descriptor\": \"Sensor Status\"}),\n",
    "            \"NumRain\": (\"time\", df[\"precip_partitioning\"].apply(lambda x: x[0])),\n",
    "            \"NumNoRain\": (\"time\", df[\"precip_partitioning\"].apply(lambda x: x[1])),\n",
    "            \"NumAmbig\": (\"time\", df[\"precip_partitioning\"].apply(lambda x: x[2])),\n",
    "            \"Type\": (\"time\", df[\"precip_partitioning\"].apply(lambda x: x[3]))\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": df[\"time\"].values,\n",
    "            \"size_bins\": size_bins,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add descriptor attributes\n",
    "    ds[\"Blackout\"].attrs[\"descriptor\"] = \"Samples\"\n",
    "    ds[\"Good\"].attrs[\"descriptor\"] = \"Samples\"\n",
    "    ds[\"Bad\"].attrs[\"descriptor\"] = \"Samples\"\n",
    "    ds[\"NumParticle\"].attrs[\"descriptor\"] = \"Precipitation\"\n",
    "    ds[\"NumParticle\"].attrs[\"units\"] = \"\"\n",
    "    ds[\"Rate\"].attrs[\"descriptor\"] = \"Precipitation\"\n",
    "    ds[\"Amount\"].attrs[\"descriptor\"] = \"Precipitation\"\n",
    "    ds[\"AmountSum\"].attrs[\"descriptor\"] = \"Precipitation\"\n",
    "    ds[\"Z\"].attrs[\"descriptor\"] = \"Precipitation\"\n",
    "    ds[\"NumError\"].attrs[\"descriptor\"] = \"Laser Status\"\n",
    "    ds[\"Dirty\"].attrs[\"descriptor\"] = \"Laser Status\"\n",
    "    ds[\"VeryDirty\"].attrs[\"descriptor\"] = \"Laser Status\"\n",
    "    ds[\"Damaged\"].attrs[\"descriptor\"] = \"Laser Status\"\n",
    "    ds[\"SignalAvg\"].attrs[\"descriptor\"] = \"Laser Status\"\n",
    "    ds[\"SignalStdDev\"].attrs[\"descriptor\"] = \"Laser Status\"\n",
    "    ds[\"TempAvg\"].attrs[\"descriptor\"] = \"Sensor Status\"\n",
    "    ds[\"TempStdDev\"].attrs[\"descriptor\"] = \"Sensor Status\"\n",
    "    ds[\"VoltAvg\"].attrs[\"descriptor\"] = \"Sensor Status\"\n",
    "    ds[\"VoltStdDev\"].attrs[\"descriptor\"] = \"Sensor Status\"\n",
    "    ds[\"HeatCurrentAvg\"].attrs[\"descriptor\"] = \"Sensor Status\"\n",
    "    ds[\"HeatCurrentStdDev\"].attrs[\"descriptor\"] = \"Sensor Status\"\n",
    "    ds[\"NumRain\"].attrs[\"descriptor\"] = \"Precipitation Partitioning\"\n",
    "    ds[\"NumNoRain\"].attrs[\"descriptor\"] = \"Precipitation Partitioning\"\n",
    "    ds[\"NumAmbig\"].attrs[\"descriptor\"] = \"Precipitation Partitioning\"\n",
    "    ds[\"Type\"].attrs[\"descriptor\"] = \"Precipitation Partitioning\"\n",
    "\n",
    "\n",
    "        # Particle distribution\n",
    "    ds[\"particle_distribution\"].attrs[\"long_name\"] = \"Partical distribution (count) binned by ClassNumber\"\n",
    "\n",
    "    # Data acquisition software quality control\n",
    "    ds[\"Blackout\"].attrs[\"long_name\"] = \"number of data samples excluded during PC clock synchronization\"\n",
    "    ds[\"Good\"].attrs[\"long_name\"] = \"number of samples that passed the quality control checks, as performed by the data acquisition software\"\n",
    "    ds[\"Bad\"].attrs[\"long_name\"] = \"number of samples that failed the quality control checks, as performed by the data acquisition software\"\n",
    "\n",
    "    # Precipitation statistics\n",
    "    ds[\"NumParticle\"].attrs[\"long_name\"] = \"total number of detected particles\"\n",
    "    ds[\"Rate\"].attrs[\"long_name\"] = \"rain rate\"\n",
    "    ds[\"Amount\"].attrs[\"long_name\"] = \"interval rain accumulation\"\n",
    "    ds[\"AmountSum\"].attrs[\"long_name\"] = \"event rain accumulation\"\n",
    "    ds[\"Z\"].attrs[\"long_name\"] = \"radar reflectivity factor\"\n",
    "\n",
    "    # Laser status\n",
    "    ds[\"NumError\"].attrs[\"long_name\"] = \"number of sample instances that were reported as dirty, very dirty, or damaged\"\n",
    "    ds[\"Dirty\"].attrs[\"long_name\"] = \"laser protective glass is dirty, but measurements are still possible\"\n",
    "    ds[\"VeryDirty\"].attrs[\"long_name\"] = \"laser protective glass is dirty, partially covered; no further usable measurements are possible\"\n",
    "    ds[\"Damaged\"].attrs[\"long_name\"] = \"laser damaged\"\n",
    "    ds[\"SignalAvg\"].attrs[\"long_name\"] = \"average signal amplitude of the laser strip\"\n",
    "    ds[\"SignalStdDev\"].attrs[\"long_name\"] = \"standard deviation of the signal amplitude of the laser strip\"\n",
    "\n",
    "    # Sensor status\n",
    "    ds[\"TempAvg\"].attrs[\"long_name\"] = \"average sensor temperature\"\n",
    "    ds[\"TempStdDev\"].attrs[\"long_name\"] = \"standard deviation of the sensor temperature\"\n",
    "    ds[\"VoltAvg\"].attrs[\"long_name\"] = \"sensor power supply voltage\"\n",
    "    ds[\"VoltStdDev\"].attrs[\"long_name\"] = \"standard deviation of the sensor power supply voltage\"\n",
    "    ds[\"HeatCurrentAvg\"].attrs[\"long_name\"] = \"average heating system current\"\n",
    "    ds[\"HeatCurrentStdDev\"].attrs[\"long_name\"] = \"standard deviation of the heating system current\"\n",
    "\n",
    "    # Precipitation partitioning\n",
    "    ds[\"NumRain\"].attrs[\"long_name\"] = \"number of particles detected as rain\"\n",
    "    ds[\"NumNoRain\"].attrs[\"long_name\"] = \"number of particles detected not as rain\"\n",
    "    ds[\"NumAmbig\"].attrs[\"long_name\"] = \"number of particles detected as ambiguous\"\n",
    "    ds[\"Type\"].attrs[\"long_name\"] = \"precipitation type (1=rain; 2=mixed; 3=snow)\"\n",
    "\n",
    "    ds['time'].attrs['long_name'] = 'Time (UTC)'\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0 of 16141\n",
      "Processing file 100 of 16141\n",
      "Processing file 200 of 16141\n",
      "Processing file 300 of 16141\n",
      "Processing file 400 of 16141\n",
      "Processing file 500 of 16141\n",
      "Processing file 600 of 16141\n",
      "Processing file 700 of 16141\n",
      "Processing file 800 of 16141\n",
      "Processing file 900 of 16141\n",
      "Processing file 1000 of 16141\n",
      "Processing file 1100 of 16141\n",
      "Processing file 1200 of 16141\n",
      "Processing file 1300 of 16141\n",
      "Processing file 1400 of 16141\n",
      "Processing file 1500 of 16141\n",
      "Processing file 1600 of 16141\n",
      "Processing file 1700 of 16141\n",
      "Processing file 1800 of 16141\n",
      "Processing file 1900 of 16141\n",
      "Processing file 2000 of 16141\n",
      "Processing file 2100 of 16141\n",
      "Processing file 2200 of 16141\n",
      "Processing file 2300 of 16141\n",
      "Processing file 2400 of 16141\n",
      "Processing file 2500 of 16141\n",
      "Processing file 2600 of 16141\n",
      "Processing file 2700 of 16141\n",
      "Processing file 2800 of 16141\n",
      "Processing file 2900 of 16141\n",
      "Processing file 3000 of 16141\n",
      "Processing file 3100 of 16141\n",
      "Processing file 3200 of 16141\n",
      "Processing file 3300 of 16141\n",
      "Processing file 3400 of 16141\n",
      "Processing file 3500 of 16141\n",
      "Processing file 3600 of 16141\n",
      "Processing file 3700 of 16141\n",
      "Processing file 3800 of 16141\n",
      "Processing file 3900 of 16141\n",
      "Processing file 4000 of 16141\n",
      "Processing file 4100 of 16141\n",
      "Processing file 4200 of 16141\n",
      "Processing file 4300 of 16141\n",
      "Processing file 4400 of 16141\n",
      "Processing file 4500 of 16141\n",
      "Processing file 4600 of 16141\n",
      "Processing file 4700 of 16141\n",
      "Processing file 4800 of 16141\n",
      "Processing file 4900 of 16141\n",
      "Processing file 5000 of 16141\n",
      "Processing file 5100 of 16141\n",
      "Processing file 5200 of 16141\n",
      "Processing file 5300 of 16141\n",
      "Processing file 5400 of 16141\n",
      "Processing file 5500 of 16141\n",
      "Processing file 5600 of 16141\n",
      "Processing file 5700 of 16141\n",
      "Processing file 5800 of 16141\n",
      "Processing file 5900 of 16141\n",
      "Processing file 6000 of 16141\n",
      "Processing file 6100 of 16141\n",
      "Processing file 6200 of 16141\n",
      "Processing file 6300 of 16141\n",
      "Processing file 6400 of 16141\n",
      "Processing file 6500 of 16141\n",
      "Processing file 6600 of 16141\n",
      "Processing file 6700 of 16141\n",
      "Processing file 6800 of 16141\n",
      "Processing file 6900 of 16141\n",
      "Processing file 7000 of 16141\n",
      "Processing file 7100 of 16141\n",
      "Processing file 7200 of 16141\n",
      "Processing file 7300 of 16141\n",
      "Processing file 7400 of 16141\n",
      "Processing file 7500 of 16141\n",
      "Processing file 7600 of 16141\n",
      "Processing file 7700 of 16141\n",
      "Processing file 7800 of 16141\n",
      "Processing file 7900 of 16141\n",
      "Processing file 8000 of 16141\n",
      "Processing file 8100 of 16141\n",
      "Processing file 8200 of 16141\n",
      "Processing file 8300 of 16141\n",
      "Processing file 8400 of 16141\n",
      "Processing file 8500 of 16141\n",
      "Processing file 8600 of 16141\n",
      "Processing file 8700 of 16141\n",
      "Processing file 8800 of 16141\n",
      "Processing file 8900 of 16141\n",
      "Processing file 9000 of 16141\n",
      "Processing file 9100 of 16141\n",
      "Processing file 9200 of 16141\n",
      "Processing file 9300 of 16141\n",
      "Processing file 9400 of 16141\n",
      "Processing file 9500 of 16141\n",
      "Processing file 9600 of 16141\n",
      "Processing file 9700 of 16141\n",
      "Processing file 9800 of 16141\n",
      "Processing file 9900 of 16141\n",
      "Processing file 10000 of 16141\n",
      "Processing file 10100 of 16141\n",
      "Processing file 10200 of 16141\n",
      "Processing file 10300 of 16141\n",
      "Processing file 10400 of 16141\n",
      "Processing file 10500 of 16141\n",
      "Processing file 10600 of 16141\n",
      "Processing file 10700 of 16141\n",
      "Processing file 10800 of 16141\n",
      "Processing file 10900 of 16141\n",
      "Processing file 11000 of 16141\n",
      "Processing file 11100 of 16141\n",
      "Processing file 11200 of 16141\n",
      "Processing file 11300 of 16141\n",
      "Processing file 11400 of 16141\n",
      "Processing file 11500 of 16141\n",
      "Processing file 11600 of 16141\n",
      "Processing file 11700 of 16141\n",
      "Processing file 11800 of 16141\n",
      "Processing file 11900 of 16141\n",
      "Processing file 12000 of 16141\n",
      "Processing file 12100 of 16141\n",
      "Processing file 12200 of 16141\n",
      "Processing file 12300 of 16141\n",
      "Processing file 12400 of 16141\n",
      "Processing file 12500 of 16141\n",
      "Processing file 12600 of 16141\n",
      "Processing file 12700 of 16141\n",
      "Processing file 12800 of 16141\n",
      "Processing file 12900 of 16141\n",
      "Processing file 13000 of 16141\n",
      "Processing file 13100 of 16141\n",
      "Processing file 13200 of 16141\n",
      "Processing file 13300 of 16141\n",
      "Processing file 13400 of 16141\n",
      "Processing file 13500 of 16141\n",
      "Processing file 13600 of 16141\n",
      "Processing file 13700 of 16141\n",
      "Processing file 13800 of 16141\n",
      "Processing file 13900 of 16141\n",
      "Processing file 14000 of 16141\n",
      "Processing file 14100 of 16141\n",
      "Processing file 14200 of 16141\n",
      "Processing file 14300 of 16141\n",
      "Processing file 14400 of 16141\n",
      "Processing file 14500 of 16141\n",
      "Processing file 14600 of 16141\n",
      "Processing file 14700 of 16141\n",
      "Processing file 14800 of 16141\n",
      "Processing file 14900 of 16141\n",
      "Processing file 15000 of 16141\n",
      "Processing file 15100 of 16141\n",
      "Processing file 15200 of 16141\n",
      "Processing file 15300 of 16141\n",
      "Processing file 15400 of 16141\n",
      "Processing file 15500 of 16141\n",
      "Processing file 15600 of 16141\n",
      "Processing file 15700 of 16141\n",
      "Processing file 15800 of 16141\n",
      "Processing file 15900 of 16141\n",
      "Processing file 16000 of 16141\n",
      "Processing file 16100 of 16141\n"
     ]
    }
   ],
   "source": [
    "# now we can process all the files\n",
    "# lets start by creating a list of all the datasets\n",
    "ds_list = []\n",
    "\n",
    "for file in files:\n",
    "    # print the run time every 100 files to keep track of progress\n",
    "    if files.index(file) % 100 == 0:\n",
    "        print(f\"Processing file {files.index(file)} of {len(files)}\")\n",
    "    try:\n",
    "        ds = process_laser_disdrometer_file(file)\n",
    "        ds_list.append(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "# then we can concatenate them all together\n",
    "combined_ds = xr.concat(ds_list, dim=\"time\")\n",
    "# lastly we can make sure its sorted by time\n",
    "combined_ds = combined_ds.sortby(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_xarray_dataset(ds, resampling_interval):\n",
    "    \"\"\"\n",
    "    Resamples an xarray Dataset by converting it to a pandas DataFrame,\n",
    "    flattening the index, performing resampling, and converting back to xarray Dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - ds (xr.Dataset): Input xarray Dataset to be resampled.\n",
    "    - resampling_interval (str): Resampling interval string (e.g., '1H', '1D', '1M').\n",
    "    \n",
    "    Returns:\n",
    "    - xr.Dataset: Resampled xarray Dataset.\n",
    "    \"\"\"\n",
    "    attrs_dataset = ds.attrs.copy()\n",
    "    attrs_vars = {var: ds[var].attrs.copy() for var in ds.variables}\n",
    "\n",
    "    # Convert dataset to pandas DataFrame and flatten the multi-index\n",
    "    df = ds.to_dataframe().reset_index()\n",
    "\n",
    "    # Define a function to calculate mode\n",
    "    def mode_function(x):\n",
    "        return x.mode().iloc[0] if not x.mode().empty else None\n",
    "    # Perform resampling in pandas using agg, all dimension except size_bins should be averaged, size_bins should be constant\n",
    "    df_resampled = df.set_index('time').groupby('size_bins').resample('1H').agg({'particle_distribution': 'mean', \n",
    "                                                                                'Blackout': mode_function, \n",
    "                                                                                'Good': mode_function, \n",
    "                                                                                'Bad': mode_function, \n",
    "                                                                                'NumParticle': 'sum', \n",
    "                                                                                'Rate': 'mean', \n",
    "                                                                                'Amount': 'sum', \n",
    "                                                                                'AmountSum': 'sum', \n",
    "                                                                                'Z': 'mean', \n",
    "                                                                                'NumError': mode_function, \n",
    "                                                                                'Dirty': mode_function, \n",
    "                                                                                'VeryDirty': mode_function, \n",
    "                                                                                'Damaged': mode_function, \n",
    "                                                                                'SignalAvg': 'mean', \n",
    "                                                                                'SignalStdDev': 'mean', \n",
    "                                                                                'TempAvg': 'mean', \n",
    "                                                                                'TempStdDev': 'mean', \n",
    "                                                                                'VoltAvg': 'mean', \n",
    "                                                                                'VoltStdDev': 'mean', \n",
    "                                                                                'HeatCurrentAvg': 'mean', \n",
    "                                                                                'HeatCurrentStdDev': 'mean', \n",
    "                                                                                'NumRain': mode_function, \n",
    "                                                                                'NumNoRain': mode_function, \n",
    "                                                                                'NumAmbig': mode_function, \n",
    "                                                                                'Type': 'mean'})\n",
    "\n",
    "    # reset the index, then make the multiindex from time and size_bins\n",
    "    df_resampled = df_resampled.reset_index()\n",
    "    # make a multiindex from time and size_bins\n",
    "    df_resampled = df_resampled.set_index(['time', 'size_bins'])\n",
    "\n",
    "    # Convert DataFrame back to xarray Dataset\n",
    "    ds_resampled = df_resampled.to_xarray()\n",
    "\n",
    "    # Restore attributes to the resampled xarray Dataset\n",
    "    ds_resampled.attrs.update(attrs_dataset)\n",
    "    for var in ds_resampled.variables:\n",
    "        ds_resampled[var].attrs.update(attrs_vars.get(var, {}))\n",
    "\n",
    "    return ds_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example = xr.concat(ds_list[0:100], dim=\"time\")\n",
    "# resample the dataset to hourly intervals using pandas\n",
    "# Preserve attributes before converting to pandas\n",
    "attrs_dataset = example.attrs.copy()\n",
    "attrs_vars = {var: example[var].attrs.copy() for var in example.variables}\n",
    "\n",
    "# Convert dataset to pandas DataFrame and flatten the multi-index\n",
    "df = example.to_dataframe().reset_index()\n",
    "\n",
    "# Define a function to calculate mode\n",
    "def mode_function(x):\n",
    "    return x.mode().iloc[0] if not x.mode().empty else None\n",
    "# Perform resampling in pandas using agg, all dimension except size_bins should be averaged, size_bins should be constant\n",
    "df_resampled = df.set_index('time').groupby('size_bins').resample('1H').agg({'particle_distribution': 'mean', \n",
    "                                                                             'Blackout': mode_function, \n",
    "                                                                             'Good': mode_function, \n",
    "                                                                             'Bad': mode_function, \n",
    "                                                                             'NumParticle': 'sum', \n",
    "                                                                             'Rate': 'mean', \n",
    "                                                                             'Amount': 'sum', \n",
    "                                                                             'AmountSum': 'sum', \n",
    "                                                                             'Z': 'mean', \n",
    "                                                                             'NumError': mode_function, \n",
    "                                                                             'Dirty': mode_function, \n",
    "                                                                             'VeryDirty': mode_function, \n",
    "                                                                             'Damaged': mode_function, \n",
    "                                                                             'SignalAvg': 'mean', \n",
    "                                                                             'SignalStdDev': 'mean', \n",
    "                                                                             'TempAvg': 'mean', \n",
    "                                                                             'TempStdDev': 'mean', \n",
    "                                                                             'VoltAvg': 'mean', \n",
    "                                                                             'VoltStdDev': 'mean', \n",
    "                                                                             'HeatCurrentAvg': 'mean', \n",
    "                                                                             'HeatCurrentStdDev': 'mean', \n",
    "                                                                             'NumRain': mode_function, \n",
    "                                                                             'NumNoRain': mode_function, \n",
    "                                                                             'NumAmbig': mode_function, \n",
    "                                                                             'Type': 'mean'})\n",
    "\n",
    "# reset the index, then make the multiindex from time and size_bins\n",
    "df_resampled = df_resampled.reset_index()\n",
    "# make a multiindex from time and size_bins\n",
    "df_resampled = df_resampled.set_index(['time', 'size_bins'])\n",
    "\n",
    "# Convert DataFrame back to xarray Dataset\n",
    "example_resampled = df_resampled.to_xarray()\n",
    "\n",
    "# Restore attributes to the resampled xarray Dataset\n",
    "example_resampled.attrs.update(attrs_dataset)\n",
    "for var in example_resampled.variables:\n",
    "    example_resampled[var].attrs.update(attrs_vars.get(var, {}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_resampled = resample_xarray_dataset(example, '5min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the 5minute resampled mean of the data by first\n",
    "# saving all the variable and dataset attributes and then converting to pands to resample\n",
    "# converting back to xarray and adding the attributes back\n",
    "# resampled_5min mean\n",
    "resampled_5min_ds = resample_xarray_dataset(combined_ds, '5min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data into water year 2022 and 2023 datasets\n",
    "wy22_ds = combined_ds.sel(time=slice(\"2021-10-01\", \"2022-09-30\"))\n",
    "wy23_ds = combined_ds.sel(time=slice(\"2022-10-01\", \"2023-09-30\"))\n",
    "# close combined_ds\n",
    "combined_ds.close()\n",
    "\n",
    "# # do the same for the resampled datasets\n",
    "wy22_resampled_5min_ds = resampled_5min_ds.sel(time=slice(\"2021-10-01\", \"2022-09-30\"))\n",
    "wy23_resampled_5min_ds = resampled_5min_ds.sel(time=slice(\"2022-10-01\", \"2023-09-30\"))\n",
    "# close resampled_5min_ds\n",
    "resampled_5min_ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample 1H mean\n",
    "resampled_1H_ds = resample_xarray_dataset(combined_ds, '1H')\n",
    "wy23_resampled_1H_ds = resampled_1H_ds.sel(time=slice(\"2022-10-01\", \"2023-09-30\"))\n",
    "wy22_resampled_1H_ds = resampled_1H_ds.sel(time=slice(\"2021-10-01\", \"2022-09-30\"))\n",
    "resampled_1H_ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset to a netcdf file\n",
    "output = True\n",
    "if output:\n",
    "    wy22_ds.to_netcdf('../../01_data/processed_data/splash/wy22_SPLASH_kp_ldis.nc')\n",
    "    wy23_ds.to_netcdf('../../01_data/processed_data/splash/wy23_SPLASH_kp_ldis.nc')\n",
    "    wy22_resampled_5min_ds.to_netcdf('../../01_data/processed_data/splash/wy22_resampled_5min_SPLASH_kp_ldis.nc')\n",
    "    wy23_resampled_5min_ds.to_netcdf('../../01_data/processed_data/splash/wy23_resampled_5min_SPLASH_kp_ldis.nc')\n",
    "    wy23_resampled_1H_ds.to_netcdf('../../01_data/processed_data/splash/wy23_resampled_1H_SPLASH_kp_ldis.nc')\n",
    "    wy22_resampled_1H_ds.to_netcdf('../../01_data/processed_data/splash/wy22_resampled_1H_SPLASH_kp_ldis.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close all open datasets\n",
    "wy22_ds.close()\n",
    "wy23_ds.close()\n",
    "wy22_resampled_5min_ds.close()\n",
    "wy23_resampled_5min_ds.close()\n",
    "wy22_resampled_1H_ds.close()\n",
    "wy23_resampled_1H_ds.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sublime_synoptics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
