{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already downloaded\n"
     ]
    }
   ],
   "source": [
    "# set the filepath\n",
    "filepath = '/storage/dlhogan/synoptic_sublimation/splash_data/'\n",
    "if not os.path.exists(filepath+'ceilometer'):\n",
    "    print('Data not downlaoded yet. Would you like to download it?')\n",
    "    download = input('y/n: ')\n",
    "    if download == 'y':\n",
    "        # downlaod the data from https://zenodo.org/records/10520198/files/ckp.cl51.cloud_prod.zip using wget to the filepath\n",
    "        os.system('wget https://zenodo.org/records/10520198/files/ckp.cl51.cloud_prod.zip -P '+filepath)\n",
    "        # unzip the file\n",
    "        os.system('unzip '+filepath+'ckp.cl51.cloud_prod.zip -d '+filepath)\n",
    "        # remove the zip file\n",
    "        os.system('rm '+filepath+'ckp.cl51.cloud_prod.zip')\n",
    "        # move 2021/ 2022/ and 2023/ folders into a new ceilometer folder\n",
    "        os.system('mkdir '+filepath+'ceilometer')\n",
    "        os.system('mv '+filepath+'2021/'+filepath+'ceilometer/')\n",
    "        os.system('mv '+filepath+'2022/'+filepath+'ceilometer/')\n",
    "        os.system('mv '+filepath+'2023/'+filepath+'ceilometer/')\n",
    "    else:\n",
    "        print('Download the data from https://zenodo.org/records/10520198/files/ckp.cl51.cloud_prod.zip')\n",
    "else:\n",
    "    print('Data already downloaded')\n",
    "    # we'll start by loading in one file and looking at the data\n",
    "    filepath = '/storage/dlhogan/synoptic_sublimation/splash_data/ceilometer/*'\n",
    "    files = glob.glob(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ceilometer_data(files):\n",
    "    concatenated_data = []\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            ds = xr.open_dataset(file)\n",
    "            \n",
    "            # Filter out range values greater than 4000\n",
    "            ds = ds.where(ds.range < 4000, drop=True)\n",
    "            \n",
    "            # Convert backscatter profile units\n",
    "            ds['backscatter_profile'] = ds['backscatter_profile'] * 1e6  # Convert from 10e-9 to 10e-3 (10e-5 m^-1 sr^-1)\n",
    "            ds['backscatter_profile'].attrs['units'] = '10e-5 m^-1 sr^-1'  # Update units attribute\n",
    "            \n",
    "            # remove the instrumen_reported_time variable\n",
    "            ds = ds.drop('instrument_reported_time')\n",
    "            # Add modified dataset to list\n",
    "            concatenated_data.append(ds)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "    \n",
    "    # Concatenate all datasets into a single dataset\n",
    "    combined_ds = xr.concat(concatenated_data, dim='time')\n",
    "    \n",
    "    return combined_ds\n",
    "\n",
    "# Example usage for the given files\n",
    "files_21 = glob.glob(filepath+'2021/*')\n",
    "files_22 = glob.glob(filepath+'2022/*')\n",
    "files_23 = glob.glob(filepath+'2023/*')\n",
    "\n",
    "# Concatenate datasets for each year\n",
    "concatenated_data_21 = process_ceilometer_data(files_21)\n",
    "concatenated_data_22 = process_ceilometer_data(files_22)\n",
    "concatenated_data_23 = process_ceilometer_data(files_23)\n",
    "\n",
    "# Optionally, concatenate across all years if needed\n",
    "concatenated_data_all = xr.concat([concatenated_data_21, concatenated_data_22, concatenated_data_23], dim='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlhogan/miniforge3/envs/sublime_synoptics/lib/python3.11/site-packages/xarray/core/rolling.py:1121: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  reduced[key] = da.variable.coarsen(\n",
      "/home/dlhogan/miniforge3/envs/sublime_synoptics/lib/python3.11/site-packages/xarray/core/rolling.py:1121: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  reduced[key] = da.variable.coarsen(\n",
      "/home/dlhogan/miniforge3/envs/sublime_synoptics/lib/python3.11/site-packages/xarray/core/rolling.py:1121: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  reduced[key] = da.variable.coarsen(\n",
      "/home/dlhogan/miniforge3/envs/sublime_synoptics/lib/python3.11/site-packages/xarray/core/rolling.py:1121: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  reduced[key] = da.variable.coarsen(\n"
     ]
    }
   ],
   "source": [
    "# Example chunking your data with dask\n",
    "ds = concatenated_data_all.chunk({'time': 'auto'})\n",
    "\n",
    "# Coarsen and compute 5-minute averages using dask, with 'boundary' set to 'trim'\n",
    "ds_coarsened = ds.coarsen(time=5, boundary='trim').mean()\n",
    "\n",
    "# Calculate the time coordinates to represent the start of each trim period\n",
    "# The start of each trim period is simply the existing time coordinates of the original dataset\n",
    "trimmed_time_coords = ds.time[:-4:5].values\n",
    "\n",
    "# Assign the calculated time coordinates to the coarsened dataset\n",
    "ds_coarsened['time'] = trimmed_time_coords\n",
    "\n",
    "# Compute the result using dask\n",
    "ds_coarsened = ds_coarsened.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 2121.899161 MB\n"
     ]
    }
   ],
   "source": [
    "# sort the dataset by time\n",
    "ds_coarsened = ds_coarsened.sortby('time')\n",
    "\n",
    "save_file = True\n",
    "if save_file:\n",
    "    ds_coarsened.to_netcdf('/storage/dlhogan/synoptic_sublimation/splash_data/ceilometer/splash_kp_ceilometer.nc')\n",
    "# print the size of the file in MB\n",
    "print(f\"File size: {os.path.getsize('/storage/dlhogan/synoptic_sublimation/splash_data/ceilometer/splash_kp_ceilometer.nc') / 1e6} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sublime_synoptics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
