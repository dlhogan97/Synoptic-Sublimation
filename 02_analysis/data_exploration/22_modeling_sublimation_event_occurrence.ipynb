{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Sublimation Event Occurrence\n",
    "\n",
    "Written by: Daniel Hogan\n",
    "\n",
    "10 April 2025\n",
    "\n",
    "It's raining outside with a weak AR-like event impacting northern VI\n",
    "\n",
    "This notebook will develop, test, and validate a few different model strategies to classify when sublimation events occur using ERA5 reanalysis data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import datetime as dt\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "from typing import Tuple, List\n",
    "\n",
    "# data \n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "# helper tools\n",
    "from metpy import calc, units\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Data driven modeling \n",
    "from sklearn.model_selection import (\n",
    "    GroupKFold,\n",
    "    ShuffleSplit,\n",
    "    GroupShuffleSplit,\n",
    "    KFold,\n",
    "    StratifiedGroupKFold,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    TimeSeriesSplit,\n",
    "    train_test_split,\n",
    "    cross_val_predict,\n",
    "    cross_validate,\n",
    "    cross_val_score,\n",
    "    ValidationCurveDisplay,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedStratifiedKFold\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "\n",
    "# stats\n",
    "import shapely.geometry as shp\n",
    "from itertools import product\n",
    "from scipy.stats import pearsonr, spearmanr, linregress\n",
    "from scipy.ndimage import label, find_objects\n",
    "import pymannkendall as mk\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and subset ERA5 data\n",
    "def load_era5_data(pressure_level=500):\n",
    "    \"\"\"\n",
    "    Load and subset ERA5 data based on latitude and longitude selections.\n",
    "    \"\"\"\n",
    "    ds = xr.open_mfdataset('/storage/dlhogan/data/raw_data/ERA5_reanalysis_western_NA_202*.nc')\n",
    "    ds = ds.sel(pressure_level=pressure_level)\n",
    "    return ds\n",
    "\n",
    "def calc_valley_aligned_wind_speed(wind_speed, wind_dir, valley_alignment):\n",
    "    \"\"\"\n",
    "    Calculate the valley aligned wind speed\n",
    "    \"\"\"\n",
    "    # calculate the angle between the wind direction and the valley alignment\n",
    "    angle = np.abs(wind_dir - valley_alignment)\n",
    "    # calculate the valley aligned wind speed\n",
    "    valley_aligned_wind_speed = wind_speed * np.cos(np.radians(angle))\n",
    "    return valley_aligned_wind_speed\n",
    "\n",
    "    # Calculate the gradients at 500-mb\n",
    "def calc_era5_gradients(ds):\n",
    "    u = ds['u']          # shape: (time, lat, lon)\n",
    "    v = ds['v']\n",
    "    temp = ds['t']\n",
    "    q = ds['q']          # specific humidity\n",
    "    zeta = ds['vo']      # relative vorticity\n",
    "    z = ds['z']          # geopotential height\n",
    "\n",
    "    # Get spatial resolution (assumes regular grid)\n",
    "    dy = np.deg2rad(ds.latitude.diff('latitude').mean()) * 6371000  # m per degree latitude\n",
    "    dx = np.deg2rad(ds.longitude.diff('longitude').mean()) * 6371000 * np.cos(np.deg2rad(ds.latitude.mean()))  # m per degree longitude\n",
    "\n",
    "    # Convert degrees to meters\n",
    "    Re = 6.371e6  # Earth radius in meters\n",
    "    lat_rad = np.deg2rad(ds.latitude.diff('latitude'))\n",
    "\n",
    "    # Calculate dx and dy in meters\n",
    "    dx = np.deg2rad(ds.longitude.diff('longitude').mean()) * Re * np.cos(np.deg2rad(ds.latitude.mean())).values\n",
    "    dy = np.abs(np.deg2rad(ds.latitude.diff('latitude').mean()) * Re).values  # m per degree latitude\n",
    "\n",
    "    # Compute gradients\n",
    "    dT_dx = (temp.differentiate('longitude') / dx) * 1000 # Convert to K/km\n",
    "    dT_dy = (temp.differentiate('latitude') / dy) * 1000 # Convert to K/km\n",
    "\n",
    "    # Compute specific humidity gradients\n",
    "    dq_dx = (q.differentiate('longitude') / dx) * 1000 # Convert to g/kg/km\n",
    "    dq_dy = (q.differentiate('latitude') / dy) * 1000 # Convert to g/kg/km\n",
    "\n",
    "    # Compute relative vorticity gradient\n",
    "    dZeta_dx = (zeta.differentiate('longitude') / dx) * 1000 # Convert to 1/s/km\n",
    "    dZeta_dy = (zeta.differentiate('latitude') / dy) * 1000 # Convert to 1/s/km\n",
    "\n",
    "    # Optionally compute gradient magnitude and advection term\n",
    "    advection_T = -(u * dT_dx + v * dT_dy)/1000 # units of K/s\n",
    "    advection_q = -(u * dq_dx + v * dq_dy)/1000 # units of g/kg/s\n",
    "    advection_zeta = -(u * dZeta_dx + v * dZeta_dy)/1000 # units of 1/s^2\n",
    "\n",
    "    # convert these to float32\n",
    "    advection_T = advection_T.astype(np.float32).squeeze()\n",
    "    advection_q = advection_q.astype(np.float32).squeeze()\n",
    "    advection_zeta = advection_zeta.astype(np.float32).squeeze()\n",
    "\n",
    "    # add the advection terms to the dataset\n",
    "    ds['advection_T'] = (('valid_time', 'latitude', 'longitude'), advection_T.data)\n",
    "    ds['advection_q'] = (('valid_time', 'latitude', 'longitude'), advection_q.data)\n",
    "    ds['advection_zeta'] = (('valid_time', 'latitude', 'longitude'), advection_zeta.data)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def subset_ds(ds, winter_slice, calc_gradients=True):\n",
    "    \"\"\"\n",
    "    Subset the dataset to the winter months\n",
    "    \"\"\"\n",
    "    # subset the data   \n",
    "    ds_subset = ds.sel(valid_time=winter_slice)\n",
    "\n",
    "    # if calc_gradients is True, calculate the gradients of the variables\n",
    "    if calc_gradients:\n",
    "        # call the calc_era5_gradients function to calculate the gradients\n",
    "        ds_subset = calc_era5_gradients(ds_subset)\n",
    "\n",
    "    # calculate the wind speed and direction\n",
    "    ds_subset['wind_speed'] = np.sqrt(ds_subset.u**2 + ds_subset.v**2)\n",
    "    # create a column with the wind direction\n",
    "    ds_subset['wind_dir'] = np.arctan2(ds_subset['v'], ds_subset['u']) * 180 / np.pi\n",
    "    # convert wind direction to compass direction\n",
    "    ds_subset['wind_dir'] = (270 - ds_subset['wind_dir']) % 360\n",
    "\n",
    "    # calculate \"northerliness\"\n",
    "    ds_subset['northerliness'] = np.cos(np.deg2rad(ds_subset['wind_dir']))\n",
    "    \n",
    "    # calculate dew point from relative humidity and temeprature\n",
    "    # convert temperature to celsius\n",
    "    ds_subset['t'] = units.units('degC') * (ds_subset['t']-273.15)\n",
    "\n",
    "    # calculate vpd\n",
    "    ds_subset['vpd'] = calc.saturation_vapor_pressure((ds_subset['t'])) - ds_subset['q'] * units.units('Pa')\n",
    "    return ds_subset.squeeze()\n",
    "\n",
    "def spatial_subset(ds, lat_range, lon_range, point=False):\n",
    "    \"\"\"\n",
    "    Subset the dataset to a specific latitude and longitude range.\n",
    "    \"\"\"\n",
    "    if point:\n",
    "        # If point is True, select the nearest point in the dataset\n",
    "        ds_subset = ds.sel(latitude=lat_range, longitude=lon_range, method='nearest')\n",
    "        return ds_subset\n",
    "    else:\n",
    "        # if point is False, select the latitude and longitude range\n",
    "        # if is a slice, use slice\n",
    "        if isinstance(lat_range, slice) and isinstance(lon_range, slice):\n",
    "            ds_subset = ds.sel(latitude=lat_range, longitude=lon_range)\n",
    "        else:\n",
    "            # If lat_range and lon_range are lists or tuples, use slice\n",
    "            lat_range = slice(lat_range[0], lat_range[1])\n",
    "            lon_range = slice(lon_range[0], lon_range[1])\n",
    "            ds_subset = ds.sel(latitude=lat_range, longitude=lon_range)\n",
    "        return ds_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERA5 data\n",
    "\n",
    "Spatial selection is performed here!! Very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ERA5 data\n",
    "ds = load_era5_data()\n",
    "\n",
    "# Define the winter slices for 2022 and 2023\n",
    "winter_2022 = slice('2021-12-01T00:00:00', '2022-03-31T23:00:00')\n",
    "winter_2023 = slice('2022-12-01T00:00:00', '2023-03-31T23:00:00')\n",
    "\n",
    "# Subset the dataset for the winter months and calculate gradients\n",
    "ds_all_winter_2022 = subset_ds(ds, winter_2022, calc_gradients=True)\n",
    "ds_all_winter_2023 = subset_ds(ds, winter_2023, calc_gradients=True)\n",
    "\n",
    "# close the dataset to free up memory\n",
    "try:\n",
    "    ds.close()\n",
    "    ds = None\n",
    "except Exception as e:\n",
    "    print(f\"Dataset already closed or error closing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geographic data and orietation of the valley\n",
    "valley_orientation = 305\n",
    "cross_valley_orientation = 215\n",
    "# get the coordingates of Grand Junction, CO\n",
    "gj = shp.Point(-108.5506, 39.0639)\n",
    "# get the coordinates of Gothic, CO\n",
    "gothic = shp.Point(-106.9898, 38.9592)\n",
    "line = shp.LineString([gj, gothic])\n",
    "\n",
    "# Define the latitude and longitude slices for the region of interest region\n",
    "lat_sel_5_by_5 = slice(39.5, 38.25)\n",
    "lon_sel_5_by_5 = slice(-107.5, -106.5)\n",
    "\n",
    "lat_sel_10_by_10 = slice(40, 37.75)\n",
    "lon_sel_10_by_10 = slice(-108.75,-106.5)\n",
    "\n",
    "lat_sel_15_by_15 = slice(40.5, 37)\n",
    "lon_sel_15_by_15 = slice(-110, -106.5)\n",
    "\n",
    "\n",
    "# Single grid-cell selection for Gothic, CO\n",
    "ds_gt_winter_2022 = spatial_subset(ds_all_winter_2022, gothic.y, gothic.x,  point=True)\n",
    "ds_gt_winter_2023 = spatial_subset(ds_all_winter_2023, gothic.y, gothic.x, point=True)\n",
    "\n",
    "# 5 by 5 degree selection\n",
    "ds_5_by_5_winter_2022 = spatial_subset(ds_all_winter_2022, lat_sel_5_by_5, lon_sel_5_by_5).mean(dim=['latitude', 'longitude'])\n",
    "ds_5_by_5_winter_2023 = spatial_subset(ds_all_winter_2023, lat_sel_5_by_5, lon_sel_5_by_5).mean(dim=['latitude', 'longitude'])\n",
    "\n",
    "# 10 by 10 degree selection\n",
    "ds_10_by_10_winter_2022 = spatial_subset(ds_all_winter_2022, lat_sel_10_by_10, lon_sel_10_by_10).mean(dim=['latitude', 'longitude'])\n",
    "ds_10_by_10_winter_2023 = spatial_subset(ds_all_winter_2023, lat_sel_10_by_10, lon_sel_10_by_10).mean(dim=['latitude', 'longitude'])\n",
    "\n",
    "# 15 by 15 degree selection\n",
    "ds_15_by_15_winter_2022 = spatial_subset(ds_all_winter_2022, lat_sel_15_by_15, lon_sel_15_by_15).mean(dim=['latitude', 'longitude'])\n",
    "ds_15_by_15_winter_2023 = spatial_subset(ds_all_winter_2023, lat_sel_15_by_15, lon_sel_15_by_15).mean(dim=['latitude', 'longitude'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the datasets to free up memory\n",
    "try:\n",
    "    ds_all_winter_2022.close()\n",
    "    ds_all_winter_2022 = None\n",
    "except Exception as e:\n",
    "    print(f\"Dataset already closed or error closing: {e}\")\n",
    "\n",
    "try:\n",
    "    ds_all_winter_2023.close()\n",
    "    ds_all_winter_2023 = None\n",
    "except Exception as e:\n",
    "    print(f\"Dataset already closed or error closing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent heat flux data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take in sublimation data\n",
    "latent_heat_flux_2022 = pd.read_csv('./01_data/processed_data/sublimation/w22_latent_heat_flux_splash_ap.csv', index_col=0, parse_dates=True)\n",
    "latent_heat_flux_2023 = pd.read_csv('./01_data/processed_data/sublimation/w23_latent_heat_flux_3m.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open large and short events dataset \n",
    "w22_large_sublimation_long_events = pd.read_csv('./01_data/processed_data/sublimation/w22_large_sublimation_long_events.csv', index_col=0, parse_dates=True)\n",
    "w22_large_sublimation_short_events = pd.read_csv('./01_data/processed_data/sublimation/w22_large_sublimation_spiky_events.csv', index_col=0, parse_dates=True)\n",
    "w23_large_sublimation_long_events = pd.read_csv('./01_data/processed_data/sublimation/w23_large_sublimation_long_events.csv', index_col=0, parse_dates=True)\n",
    "w23_large_sublimation_short_events = pd.read_csv('./01_data/processed_data/sublimation/w23_large_sublimation_spiky_events.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation data (from Gothic weighing bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open precipitation data\n",
    "w22_precipitation = pd.read_csv('./01_data/processed_data/precipitation/w22_all_precip_30min.csv', index_col=0, parse_dates=True)['SAIL_gts_pluvio'].loc['2021-12-01':'2022-03-31']\n",
    "w23_precipitation = pd.read_csv('./01_data/processed_data/precipitation/w23_all_precip_30min.csv', index_col=0, parse_dates=True)['SAIL_gts_pluvio'].loc['2022-12-01':'2023-03-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long term observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GJT sounding 500-mb winds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations from GUC sounding\n",
    "guc_obs = pd.read_csv('./01_data/processed_data/winter_500mb_GUC_winds.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long-term Dec-Mar precipitation from billy barr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO GET THIS!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_correlation(df, target_var, lag_range=(-10, 10)):\n",
    "    \"\"\"\n",
    "    Compute cross-correlations between a target variable and other variables with different time lags.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with time-indexed meteorological variables.\n",
    "    - target_var: Name of the target variable (e.g., \"sublimation\").\n",
    "    - lag_range: Tuple specifying min and max lag (e.g., (-10, 10)).\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing cross-correlations for each lag.\n",
    "    \"\"\"\n",
    "    variables = df.columns.drop(target_var)  # Exclude the target variable itself\n",
    "    lag_values = range(lag_range[0], lag_range[1] + 1)\n",
    "\n",
    "    correlation_results = pd.DataFrame(index=lag_values, columns=variables)\n",
    "\n",
    "    for lag in lag_values:\n",
    "        shifted_target = df[target_var].shift(lag)  # Shift target variable\n",
    "        correlation_results.loc[lag] = df[variables].corrwith(shifted_target)\n",
    "\n",
    "    return correlation_results.astype(float)\n",
    "\n",
    "def find_best_synoptic_variable(surface_obs, synoptic_data, method='pearson'):\n",
    "    \"\"\"\n",
    "    Finds the synoptic-scale variable that is most closely related to a given surface meteorology observation.\n",
    "\n",
    "    Parameters:\n",
    "    - surface_obs (pd.Series): Time series of the surface meteorology variable (e.g., temperature, humidity).\n",
    "    - synoptic_data (pd.DataFrame): Time series of synoptic-scale variables (e.g., geopotential height, wind speed).\n",
    "    - method (str): Correlation method ('pearson' for linear, 'spearman' for rank-based). Default is 'pearson'.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Contains the best-correlated variable, its correlation coefficient, and the full correlation results.\n",
    "    \"\"\"\n",
    "    \n",
    "    correlations = {}\n",
    "\n",
    "    for var in synoptic_data.columns:\n",
    "        synoptic_var = synoptic_data[var]\n",
    "\n",
    "        # Drop NaN values\n",
    "        valid_idx = surface_obs.index.intersection(synoptic_var.dropna().index)\n",
    "        x = surface_obs.loc[valid_idx]\n",
    "        y = synoptic_var.loc[valid_idx]\n",
    "\n",
    "        if len(x) < 10:  # Require a minimum number of valid data points\n",
    "            continue\n",
    "\n",
    "        # Compute correlation\n",
    "        if method == 'pearson':\n",
    "            corr, _ = pearsonr(x, y)\n",
    "        elif method == 'spearman':\n",
    "            corr, _ = spearmanr(x, y)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'pearson' or 'spearman'.\")\n",
    "\n",
    "        correlations[var] = corr\n",
    "\n",
    "    # Find the variable with the highest absolute correlation\n",
    "    best_var = max(correlations, key=lambda k: abs(correlations[k]))\n",
    "    \n",
    "    return {\n",
    "        'best_variable': best_var,\n",
    "        'best_correlation': correlations[best_var],\n",
    "        'all_correlations': correlations\n",
    "    }\n",
    "\n",
    "def select_data(lh_df, ppt_data, reanalysis_ds, short_events, long_events):\n",
    "    \"\"\"\n",
    "    This function will provide the necessary data for the analysis of sublimation events in relation to the reanalysis data.\n",
    "    \"\"\"\n",
    "    # resample latent heat flux data and precipitation data to 3H\n",
    "    lh_df_3H = lh_df.resample('3H').mean()\n",
    "    ppt_data_3H = ppt_data.resample('3H').sum()\n",
    "    #   filter out exrtreme values\n",
    "    lh_df_3H = lh_df_3H.where(lh_df_3H >-50, np.nan)\n",
    "    # select the reanalysis pressure level\n",
    "    ds = reanalysis_ds.resample(valid_time='3H').mean()\n",
    "\n",
    "    return lh_df_3H, ppt_data_3H, ds, short_events, long_events\n",
    "\n",
    "# Plotting \n",
    "def plot_surface_and_reanalysis_vars(lh_df, ppt_data, era5_ds, short_events, long_events, lh_var, reanalysis_var1, reanalysis_var2):\n",
    "    \"\"\"\n",
    "    This function will plot the relationship between the latent heat flux and the reanalysis variables.\n",
    "    Inputs:\n",
    "    lh_df: DataFrame with the latent heat flux data\n",
    "    ppt_data: DataFrame with the precipitation data\n",
    "    era5_ds: xarray dataset with the reanalysis data\n",
    "    short_events: DataFrame with the short, intense sublimation events\n",
    "    long_events: DataFrame with the long sublimation events\n",
    "    lh_var: string with the name of the latent heat flux variable\n",
    "    reanalysis_var1: string with the name of the first reanalysis variable\n",
    "    reanalysis_var2: string with the name of the second reanalysis variable\n",
    "    \"\"\"\n",
    "    lh_df, ppt_data, era5_ds, short_events, long_events = select_data(lh_df, ppt_data, era5_ds, short_events, long_events)\n",
    "\n",
    "    xlabel_dict = {'u': 'Wind Speed (m/s)', \n",
    "                   'v': 'Wind Speed (m/s)', \n",
    "                   't': 'Temperature (C)', \n",
    "                   'q': 'Specifc Humidity (kg/kg)', \n",
    "                   'wind_speed': 'Wind Speed (m/s)', \n",
    "                   'wind_dir': 'Wind Direction (degrees)',\n",
    "                   'northerliness': 'Index of how northerly wind is (+1 strong north wind, -1 strong south wind)',\n",
    "                   }\n",
    "    xlims_dict = {\n",
    "             'q': (0, 0.0035),\n",
    "             't': (-40, -5),}\n",
    "             \n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    ax = axs[0]\n",
    "    ax.scatter(era5_ds[reanalysis_var1], lh_df[lh_var])\n",
    "    # ax.set_xlabel('Wind Speed')\n",
    "    ax.set_ylabel('Daily Average Latent Heat Flux (W/m^2)')\n",
    "    if reanalysis_var1 in xlims_dict.keys():\n",
    "            ax.set_xlim(xlims_dict[reanalysis_var1])\n",
    "\n",
    "    # add edge color around the large sublimation events \n",
    "    ax.scatter(era5_ds.sel(valid_time=long_events.index)[reanalysis_var1], lh_df.loc[long_events.index][lh_var], c=\"black\", edgecolors='black')\n",
    "    ax.scatter(era5_ds.sel(valid_time=ppt_data[ppt_data>1].index)[reanalysis_var1], lh_df.loc[ppt_data[ppt_data>1].index][lh_var], c=\"red\", edgecolors='red', label='Precipitation')\n",
    "    ax.set_xlabel(xlabel_dict[reanalysis_var1])\n",
    "    ax.axvline(28, color='black', linestyle='--')\n",
    "    \n",
    "    ax = axs[1]\n",
    "    ax.scatter(era5_ds[reanalysis_var2], lh_df[lh_var])\n",
    "    ax.scatter(era5_ds.sel(valid_time=long_events.index)[reanalysis_var2], lh_df.loc[long_events.index][lh_var], c=\"black\", edgecolors='black', label='Long Events')\n",
    "    ax.axvline(33, color='black', linestyle='--')\n",
    "    ax.scatter(era5_ds.sel(valid_time=ppt_data[ppt_data>1].index)[reanalysis_var2], lh_df.loc[ppt_data[ppt_data>1].index][lh_var], c=\"red\", edgecolors='red', label='Precipitation')\n",
    "    ax.set_xlabel(xlabel_dict[reanalysis_var2])\n",
    "    ax.set_ylabel('Daily Average Latent Heat Flux (W/m^2)')\n",
    "    ax.legend()\n",
    "    if reanalysis_var2 in xlims_dict.keys():\n",
    "            ax.set_xlim(xlims_dict[reanalysis_var2])\n",
    "    # add colorbar\n",
    "    # plt.colorbar(ax.collections[0], ax=axs, orientation='horizontal', label='500 mb Temperature (C)')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_ylim(-10, lh_df[lh_var].max()+10)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_events_col(df, \n",
    "                   short_events, \n",
    "                   long_events):\n",
    "    \"\"\"\n",
    "    This function will add a column to the dataframe indicating whether the event is a short or long event.\n",
    "    \"\"\"\n",
    "    # add event column, but only for the long sublimation events\n",
    "    df['event'] = 0\n",
    "    if long_events is not None:\n",
    "        df.loc[long_events, 'event'] = 1\n",
    "    else:\n",
    "        print(\"No long events provided.\")\n",
    "    if short_events is not None:\n",
    "        df.loc[short_events, 'event'] = 1\n",
    "    else:\n",
    "        print(\"No short events provided.\")\n",
    "    return df\n",
    "\n",
    "def create_df_combined(ds1, ds2, variables, resample=True, vars_to_add=None):\n",
    "    # merge datasets\n",
    "    ds_combined = xr.merge([ds1, ds2])\n",
    "\n",
    "    # convert to dataframe\n",
    "    df_combined = ds_combined[variables].to_dataframe()[variables]\n",
    "\n",
    "    # if vars_to_add is not None, add the key as the name and the value as the value\n",
    "    if vars_to_add is not None:\n",
    "        for key, value in vars_to_add.items():\n",
    "            if key == \"latent_heat_flux\":\n",
    "                # if the value is a dataframe, resample to 3H and take the mean\n",
    "                if isinstance(value, pd.DataFrame):\n",
    "                    df_combined = pd.merge(value, df_combined, left_index=True, right_index=True)\n",
    "                else:\n",
    "                    print(f\"{key} is not a dataframe. Make sure it is.\")\n",
    "            else:\n",
    "                df_combined[key] = value\n",
    "    # if resample is True, resample to 3H and take the mean\n",
    "    if isinstance(resample, bool) and resample:\n",
    "        df_combined = df_combined.resample('3H').mean()\n",
    "        # add event column\n",
    "        df_combined = add_events_col(df_combined, \n",
    "                                     short_events=None, \n",
    "                                     long_events=pd.concat([w22_large_sublimation_long_events,\n",
    "                                                            w23_large_sublimation_long_events[:-1]]).index)\n",
    "    elif isinstance(resample, str):\n",
    "        df_combined = df_combined.resample('3H').mean()\n",
    "        df_combined = add_events_col(df_combined, \n",
    "                                     short_events=None, \n",
    "                                     long_events=pd.concat([w22_large_sublimation_long_events,\n",
    "                                                            w23_large_sublimation_long_events[:-1]]).index)\n",
    "        df_combined = df_combined.resample(resample).agg({'sublimation': 'sum', \n",
    "                                                            'u': 'mean', \n",
    "                                                            'v': 'mean', \n",
    "                                                            't': 'mean',\n",
    "                                                            'q' : 'min',\n",
    "                                                            'vpd': 'min',\n",
    "                                                            'wind_speed': 'max', \n",
    "                                                            'precipitation':'sum',\n",
    "                                                            'advection_T': 'mean',\n",
    "                                                            'advection_q': 'mean',\n",
    "                                                            'advection_zeta': 'mean', \n",
    "                                                            'event': 'max'})\n",
    "    # reset the index and dropna\n",
    "    df_combined = df_combined.reset_index()\n",
    "    # if event is in the columns\n",
    "    if 'event' in df_combined.columns:\n",
    "        # replace event values equal to 0 with \"non-event\" and 1 with \"event\"\n",
    "        df_combined['event'] = df_combined['event'].replace({0: 'non-event', 1: 'event'})\n",
    "    else:\n",
    "        print(\"No event column found.\")\n",
    "     \n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Heat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022\n",
    "if 'splash_ap' in latent_heat_flux_2022.columns:\n",
    "    latent_heat_flux_2022.rename(columns={'splash_ap': 'sublimation'}, inplace=True)\n",
    "# 2023\n",
    "if 'sos_3m' in latent_heat_flux_2023.columns:\n",
    "    latent_heat_flux_2023.rename(columns={'sos_3m': 'sublimation'}, inplace=True)\n",
    "# combine the latent heat flux data\n",
    "latent_heat_flux = pd.concat([latent_heat_flux_2022, latent_heat_flux_2023], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add precipitation\n",
    "precip_df = pd.concat([w22_precipitation, w23_precipitation], axis=0).resample('3H').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "selected_datasets = [ds_5_by_5_winter_2022, ds_5_by_5_winter_2023]\n",
    "variables = ['t', 'wind_speed','vpd', 'u','v','q','advection_T', 'advection_q', 'advection_zeta']\n",
    "# merge the two datasets\n",
    "combined_df = create_combined_df(selected_datasets[0], \n",
    "                                 selected_datasets[1], \n",
    "                                 variables, \n",
    "                                 resample=True, \n",
    "                                 vars_to_add={'precipitation': precip_df,\n",
    "                                              'latent_heat_flux': latent_heat_flux})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "combined_df_12H = create_combined_df(selected_datasets[0],\n",
    "                                      selected_datasets[1], \n",
    "                                      variables, \n",
    "                                      resample='12H', \n",
    "                                      vars_to_add={'precipitation': precip_df,\n",
    "                                              'latent_heat_flux': latent_heat_flux})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics\n",
    "\n",
    "#### Calculate correlations between variables\n",
    "*Note: Can adjust easily for 2023, but we see similar results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use just the ERA5 grid-cell directly over Gothic\n",
    "lh_df, ppt_data, era5_ds, short_events, long_events = select_data(latent_heat_flux_2022, \n",
    "                                                                  w22_precipitation, \n",
    "                                                                  ds_5_by_5_winter_2022, \n",
    "                                                                  w22_large_sublimation_short_events, \n",
    "                                                                  w22_large_sublimation_long_events,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# largest_events = (long_events.groupby('event').mean() > 15)\n",
    "synoptic_data = era5_ds.to_dataframe()[variables].loc[[t[0] for t in long_events.groupby('event').idxmax().values]]\n",
    "surface_obs = long_events.groupby('event').max().set_index(synoptic_data.index).splash_ap\n",
    "\n",
    "result = find_best_synoptic_variable(surface_obs, synoptic_data, method='spearman')\n",
    "\n",
    "print(\"Best correlated synoptic variable:\", result['best_variable'])\n",
    "for var, corr in result['all_correlations'].items():\n",
    "    print(f\"{var}: {corr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correlations between variables using average of all grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between variables using average of all grids\n",
    "lh_df, ppt_data, era5_ds, short_events, long_events = select_data(latent_heat_flux_2022, \n",
    "                                                                  w22_precipitation, \n",
    "                                                                  ds_gt_winter_2022, \n",
    "                                                                  w22_large_sublimation_short_events, \n",
    "                                                                  w22_large_sublimation_long_events,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# largest_events = (long_events.groupby('event').mean() > 15)\n",
    "synoptic_data = era5_ds.to_dataframe()[variables].loc[[t[0] for t in long_events.groupby('event').idxmax().values]]\n",
    "surface_obs = long_events.groupby('event').max().set_index(synoptic_data.index).splash_ap\n",
    "\n",
    "result = find_best_synoptic_variable(surface_obs, synoptic_data, method='spearman')\n",
    "\n",
    "print(\"Best correlated synoptic variable:\", result['best_variable'])\n",
    "for var, corr in result['all_correlations'].items():\n",
    "    print(f\"{var}: {corr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "We averaged the synoptic level (500 mb) data in two differnt ways: the average of a 10x10 ERA5 grid centered over the East River valley and the nearest grid cell over Gothic. We took this data and calculated spearman correlations with the maximum surface latent heat flux observation during long events. \n",
    "\n",
    "We found that both the single grid cell and the regional average both correlate well with u-wind and wind-speed. However, large differences are observed for temperature (> for single grid cell), vorticity (> for grid-average)\n",
    "\n",
    "This likely means that including certain variables may not be super helpful. This is only for the 2023 data.\n",
    "\n",
    "We get a pretty different story during 2022. For the gridded average, v, q, and t have the largest relationships, with wind speed being much lower. This might be why including these other variables prove to be useful within the model. It seems q, t, u, v, and vo are the most useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentile rank figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Calculate percentiles\n",
    "percentiles = combined_df.rank(method=\"average\", pct=True)[['t', 'wind_speed','vpd', 'u','v','q','advection_T', 'advection_q', 'advection_zeta','sublimation']]\n",
    "\n",
    "# Subset for different sublimation thresholds\n",
    "percentiles_sub_50 = percentiles[percentiles['sublimation'] > 0.75]\n",
    "percentiles_sub_75 = percentiles[percentiles['sublimation'] > 0.85]\n",
    "percentiles_sub_95 = percentiles[percentiles['sublimation'] > 0.95]\n",
    "\n",
    "# Drop 'sublimation' since it's the target variable\n",
    "variables = ['t', 'wind_speed','vpd', 'u','v','q','advection_T', 'advection_q', 'advection_zeta']\n",
    "\n",
    "# Create a grouped DataFrame for boxplot\n",
    "melted_50 = percentiles_sub_50[variables].melt(var_name=\"Variable\", value_name=\"Percentile\")\n",
    "melted_50[\"Threshold\"] = \"> 70%\"\n",
    "\n",
    "melted_75 = percentiles_sub_75[variables].melt(var_name=\"Variable\", value_name=\"Percentile\")\n",
    "melted_75[\"Threshold\"] = \"> 85%\"\n",
    "\n",
    "melted_95 = percentiles_sub_95[variables].melt(var_name=\"Variable\", value_name=\"Percentile\")\n",
    "melted_95[\"Threshold\"] = \"> 95%\"\n",
    "\n",
    "# Combine into a single dataframe\n",
    "plot_data = pd.concat([melted_50, melted_75, melted_95])\n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"Variable\", y=\"Percentile\", hue=\"Threshold\", data=plot_data, palette=[\"lightblue\", \"orange\", \"red\"])\n",
    "\n",
    "# Customization\n",
    "plt.title(\"Boxplot of Meteorological Variables for Different Sublimation Thresholds\")\n",
    "plt.xlabel(\"Meteorological Variable\")\n",
    "plt.ylabel(\"Percentile Rank\")\n",
    "plt.legend(title=\"Sublimation Threshold\", loc='upper center')\n",
    "plt.xticks(rotation=30)  # Rotate x labels for better readability\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Calculate percentiles\n",
    "percentiles = combined_df.rank(method=\"average\", pct=True)[['t', 'wind_speed','vpd', 'u','v','q','advection_T', 'advection_q', 'advection_zeta','sublimation']]\n",
    "\n",
    "# Subset for different sublimation thresholds\n",
    "percentiles_event = percentiles[combined_df['event']=='event']\n",
    "percentiles_non_event = percentiles[combined_df['event']=='non-event']\n",
    "\n",
    "# Drop 'sublimation' since it's the target variable\n",
    "variables = ['t', 'wind_speed','vpd', 'u','v','q','advection_T', 'advection_q', 'advection_zeta']\n",
    "\n",
    "# Create a grouped DataFrame for boxplot\n",
    "melted_non_event = percentiles_non_event[variables].melt(var_name=\"Variable\", value_name=\"Percentile\")\n",
    "melted_non_event[\"Threshold\"] = \"non-event\"\n",
    "\n",
    "melted_event = percentiles_event[variables].melt(var_name=\"Variable\", value_name=\"Percentile\")\n",
    "melted_event[\"Threshold\"] = \"event\"\n",
    "\n",
    "# Combine into a single dataframe\n",
    "plot_data = pd.concat([melted_non_event, melted_event]).reset_index(drop=True)\n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"Variable\", y=\"Percentile\", hue=\"Threshold\", data=plot_data, palette=[\"orange\", \"red\"])\n",
    "\n",
    "# Customization\n",
    "plt.title(\"Boxplot of Meteorological Variables for Different Sublimation Thresholds\")\n",
    "plt.xlabel(\"Meteorological Variable\")\n",
    "plt.ylabel(\"Percentile Rank\")\n",
    "plt.legend(title=\"Sublimation Threshold\")\n",
    "plt.xticks(rotation=15)  # Rotate x labels for better readability\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal cross-correlation figures\n",
    "When are variables cross correlated with high sublimation rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Example usage\n",
    "cross_corrs = compute_cross_correlation(combined_df[['t', 'wind_speed','vpd', 'u','v','q','advection_T', 'advection_q', 'advection_zeta','sublimation']], \n",
    "                                        target_var='sublimation', \n",
    "                                        lag_range=(-12, 12))\n",
    "\n",
    "# Plot correlation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for var in cross_corrs.columns:\n",
    "    plt.plot(cross_corrs.index, cross_corrs[var], label=var, marker='o', linestyle='-')\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.8)  # Reference line at 0\n",
    "# change out the x-axis for hours by multiplying by 3\n",
    "plt.xticks(np.arange(-12, 13, 1), labels=np.arange(-12, 13, 1)*3)\n",
    "plt.xlabel(\"Time Lag (Hours)\")\n",
    "plt.ylabel(\"Correlation Coefficient\")\n",
    "plt.title(\"Cross-Correlation Between Reanalysis Variables and Sublimation\")\n",
    "plt.legend(title=\"Variables\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim(-0.5,0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Example usage\n",
    "cross_corrs = compute_cross_correlation(combined_df_12H[['t', 'wind_speed','vpd', 'u','v','q','advection_T', 'advection_q', 'advection_zeta','sublimation']], \n",
    "                                        target_var='sublimation', \n",
    "                                        lag_range=(-6, 6))\n",
    "\n",
    "# Plot correlation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for var in cross_corrs.columns:\n",
    "    plt.plot(cross_corrs.index, cross_corrs[var], label=var, marker='o', linestyle='-')\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.8)  # Reference line at 0\n",
    "# change out the x-axis for hours by multiplying by 3\n",
    "plt.xticks(np.arange(-6, 7, 1), labels=np.arange(-6, 7, 1)/2)\n",
    "plt.xlabel(\"Time Lag (Days)\")\n",
    "plt.ylabel(\"Correlation Coefficient\")\n",
    "plt.title(\"Cross-Correlation Between Reanalysis Variables and Sublimation\")\n",
    "plt.legend(title=\"Variables\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim(-0.5,0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "vars_to_plot = ['t', 'wind_speed','vpd', 'u','v','q','advection_T', 'advection_q', 'advection_zeta','sublimation']\n",
    "\n",
    "# plot histograms of each variable in vars_to_plot\n",
    "fig, axs = plt.subplots(5, 2, figsize=(12, 12))\n",
    "for ax, var in zip(axs.flat, vars_to_plot):\n",
    "    sns.histplot(combined_df[var], ax=ax, kde=True)\n",
    "    ax.set_title(f\"Histogram of {var}\")\n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event frequency relative to all 3H observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# create distribution of events and non-events\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.bar(combined_df['event'].value_counts().index, combined_df['event'].value_counts().values, color=['blue', 'red'], alpha=0.7)\n",
    "# add percentage of events and non-events\n",
    "for i, count in enumerate(combined_df['event'].value_counts().values):\n",
    "    ax.text(i, count, f\"{count} ({count/combined_df.shape[0]*100:.2f}%)\", ha='center', va='bottom')\n",
    "ax.set_title(\"Distribution of Events and Non-Events\")\n",
    "ax.set_xlabel(\"Event\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xticklabels([\"Non-Event\", \"Event\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to save results in the specified directory\n",
    "log_dir = '/home/dlhogan/GitHub/Synoptic-Sublimation/03_results/model_logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, 'model_results.log')\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# split helper ------------------------------------------------------\n",
    "# ------------------------------------------------------------------\n",
    "def split_train_test(data_X, data_y, test_size=0.2, random_state=42):\n",
    "    \"\"\"Split data into train and test sets.\"\"\"\n",
    "    return train_test_split(data_X, data_y,\n",
    "                            test_size=test_size,\n",
    "                            random_state=random_state)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# main training / evaluation routine -------------------------------\n",
    "# ------------------------------------------------------------------\n",
    "def train_and_evaluate_model(model, model_name,\n",
    "                             X_train, y_train, X_val, y_val,\n",
    "                             param_grid=None, search_method=\"grid\",\n",
    "                             scoring=None, cv=None):\n",
    "    \"\"\"\n",
    "    Train `model`, optionally tune hyper‑parameters, and visualise:\n",
    "      • confusion matrix          (always)\n",
    "      • feature importance barplot(if supported)\n",
    "    \"\"\"\n",
    "    # ---------- defaults ------------------------------------------\n",
    "    if scoring is None:\n",
    "        scoring = {\"F1\": \"f1\",\n",
    "                   \"Average_Precision\": metrics.make_scorer(metrics.average_precision_score)}\n",
    "    if cv is None:\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    best_params, best_score = None, None\n",
    "\n",
    "    # ---------- hyper‑parameter search ---------------------------\n",
    "    if search_method in {\"grid\", \"random\"} and param_grid:\n",
    "        search_cls = GridSearchCV if search_method == \"grid\" else RandomizedSearchCV\n",
    "        search = search_cls(estimator=model,\n",
    "                            param_grid=param_grid if search_method == \"grid\" else None,\n",
    "                            param_distributions=param_grid if search_method == \"random\" else None,\n",
    "                            scoring=scoring,\n",
    "                            refit=\"F1\",\n",
    "                            cv=cv,\n",
    "                            n_jobs=4,\n",
    "                            random_state=42 if search_method == \"random\" else None,\n",
    "                            return_train_score=True)\n",
    "        search.fit(X_train, y_train)\n",
    "        model        = search.best_estimator_\n",
    "        best_params  = search.best_params_\n",
    "        best_score   = search.best_score_\n",
    "        logging.info(f\"Best params: {best_params}\")\n",
    "        logging.info(f\"Best CV F1 : {best_score:.4f}\")\n",
    "\n",
    "    elif search_method == \"optuna\":\n",
    "        def objective(trial):\n",
    "            # ---- suggest params by model type --------------------\n",
    "            if isinstance(model, LogisticRegression):\n",
    "                C = trial.suggest_float(\"C\", 1e-4, 1e2, log=True)\n",
    "                p = \"l2\" # trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "                if p == \"l1\":\n",
    "                    s = \"liblinear\"\n",
    "                elif p == \"l2\":\n",
    "                    s = \"lbfgs\"\n",
    "                clf = LogisticRegression(C=C, penalty=p, solver=s, max_iter=1000)\n",
    "            elif isinstance(model, RandomForestClassifier):\n",
    "                clf = RandomForestClassifier(\n",
    "                    n_estimators     = trial.suggest_int(\"n_estimators\", 30, 150),\n",
    "                    max_depth        = trial.suggest_int(\"max_depth\", 3, 15),\n",
    "                    min_samples_split= trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "                    max_features     = trial.suggest_categorical(\"max_features\",\n",
    "                                                                 [\"sqrt\", \"log2\", 3, 4]),\n",
    "                    n_jobs=12)\n",
    "            elif isinstance(model, XGBClassifier):\n",
    "                clf = XGBClassifier(\n",
    "                        max_depth        = trial.suggest_int(\"max_depth\", 4, 8),\n",
    "                        learning_rate    = trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "                        n_estimators     = trial.suggest_int(\"n_estimators\", 30, 150),\n",
    "                        subsample        = trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                        scale_pos_weight = trial.suggest_float(\"scale_pos_weight\", 5, 15),  # around imbalance ratio\n",
    "                        reg_alpha        = trial.suggest_float(\"reg_alpha\", 0, 0.5),\n",
    "                        reg_lambda       = trial.suggest_float(\"reg_lambda\", 0.5, 2.0),\n",
    "                        # use_label_encoder=False,\n",
    "                        eval_metric=\"logloss\",\n",
    "                        n_jobs=12)\n",
    "            else:\n",
    "                raise ValueError(\"Optuna not configured for this model type\")\n",
    "\n",
    "            return cross_val_score(clf, X_train, y_train,\n",
    "                                   cv=cv, scoring=\"f1\").mean()\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=20)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_score  = study.best_value\n",
    "        logging.info(f\"Best params (Optuna): {best_params}\")\n",
    "        logging.info(f\"Best CV F1 (Optuna): {best_score:.4f}\")\n",
    "\n",
    "        # rebuild the model with best parameters\n",
    "        model.set_params(**best_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    # ---------- validation evaluation ---------------------------\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred       = model.predict(X_val)\n",
    "\n",
    "    f1_score_val      = metrics.f1_score(y_val, y_pred)\n",
    "    average_precision = metrics.average_precision_score(y_val, y_pred_proba)\n",
    "    baseline_precision= y_val.mean()\n",
    "    roc_auc_val       = metrics.roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "    logging.info(f\"{model_name} | F1: {f1_score_val:.4f}  AP: {average_precision:.4f}  \"\n",
    "                 f\"ROC‑AUC: {roc_auc_val:.4f}\")\n",
    "\n",
    "    # ---------- visualisations ----------------------------------\n",
    "    ConfusionMatrixDisplay.from_estimator(model, X_val, y_val,\n",
    "                                          cmap=\"Blues\", colorbar=False,\n",
    "                                          display_labels=[\"non‑event\", \"event\"])\n",
    "    plt.title(f\"{model_name} – Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # ---- feature importance (if available) ---------------------\n",
    "    # tree models: feature_importances_ ; logistic: coef_\n",
    "    importances = None\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        importances = np.abs(model.coef_.ravel())  # magnitude for LR\n",
    "\n",
    "    if importances is not None:\n",
    "        # obtain feature names or fallback to generic\n",
    "        if hasattr(X_train, \"columns\"):\n",
    "            feature_names = X_train.columns\n",
    "        else:\n",
    "            feature_names = [f\"f{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "        # sort by importance\n",
    "        idx_sorted = np.argsort(importances)[::-1]\n",
    "        top_n = min(20, len(idx_sorted))           # show at most 20 for clarity\n",
    "        idx_sorted = idx_sorted[:top_n]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.barh(range(top_n), importances[idx_sorted][::-1])\n",
    "        plt.yticks(range(top_n), np.array(feature_names)[idx_sorted][::-1])\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.title(f\"{model_name} – Top {top_n} Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # ---------- return summary ----------------------------------\n",
    "    return {\"model\":               model,\n",
    "            \"best_params\":         best_params,\n",
    "            \"best_score\":          best_score,\n",
    "            \"f1_score\":            f1_score_val,\n",
    "            \"average_precision\":   average_precision,\n",
    "            \"baseline_precision\":  baseline_precision,\n",
    "            \"roc_auc\":             roc_auc_val}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test/Validation Split\n",
    "\n",
    "Very important, check test size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "test_size = 0.1\n",
    "# Establish model select variables that will be used to train the model.\n",
    "selected_datasets = [ds_10_by_10_winter_2022, \n",
    "                     ds_10_by_10_winter_2023]\n",
    "model_variables = ['t', 'wind_speed','vpd', 'u','v','q','advection_T', 'advection_q', 'advection_zeta']\n",
    "# merge the two datasets\n",
    "combined_df = create_combined_df(selected_datasets[0], \n",
    "                                 selected_datasets[1], \n",
    "                                 variables, \n",
    "                                 resample=True, \n",
    "                                 vars_to_add={'precipitation': precip_df,\n",
    "                                              'latent_heat_flux': latent_heat_flux})\n",
    "\n",
    "data_X = combined_df.dropna(how='any')[model_variables]\n",
    "data_y = combined_df.dropna(how='any')['event'].replace({'non-event': 0, 'event': 1})    \n",
    "\n",
    "X_train, X_val, y_train, y_val = split_train_test(data_X, data_y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for single variable logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish model select variables that will be used to train the model.\n",
    "single_model_variables = ['wind_speed']\n",
    "\n",
    "data_X_single = combined_df.dropna(how='any')[single_model_variables]\n",
    "data_y_single = combined_df.dropna(how='any')['event'].replace({'non-event': 0, 'event': 1})    \n",
    "\n",
    "X_train_single, X_val_single, y_train_single, y_val_single = split_train_test(data_X_single, data_y_single, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Cross-Validation\n",
    "Ceate cross-validation object consisting of 10 folds using stratified sampling and shufffle given that this is a binary classification problem. A variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. I am shuffling only once to make sure that time series of events does not influence the model result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define models and parameter grids\n",
    "models = {\n",
    "    \"Logistic Regression (Single)\": {\n",
    "        \"model\": LogisticRegression(class_weight='balanced', random_state=42),\n",
    "        \"param_grid\": {\n",
    "            \"C\": [0.01, 0.1, 1, 10],\n",
    "            \"penalty\": [\"l1\", \"l2\"],\n",
    "            \"solver\": [\"liblinear\", \"saga\"]\n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(class_weight='balanced', random_state=42),\n",
    "        \"param_grid\": {\n",
    "            \"C\": [0.01, 0.1, 1, 10],\n",
    "            \"penalty\": [\"l1\", \"l2\"],\n",
    "            \"solver\": [\"liblinear\", \"saga\"]\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [50, 100, 150],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(random_state=42, objective=\"binary:logistic\"),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [40, 50, 60],\n",
    "            \"scale_pos_weight\": [10, 20, 30],\n",
    "            \"learning_rate\": [0.1, 0.2, 0.3],\n",
    "            \"max_depth\": [5, 7, 10],\n",
    "            \"subsample\": [0.5, 0.7, 0.9]\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\"F1\": \"f1\", \"Average_Precision\": metrics.make_scorer(metrics.average_precision_score)}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for model_name, config in models.items():\n",
    "    if model_name == \"Logistic Regression (Single)\":\n",
    "        results[model_name] = train_and_evaluate_model(\n",
    "            model=config[\"model\"],\n",
    "            model_name=model_name,\n",
    "            X_train=X_train_single,\n",
    "            y_train=y_train_single,\n",
    "            X_val=X_val_single,\n",
    "            y_val=y_val_single,\n",
    "            param_grid=None, # set to None to use Optuna\n",
    "            search_method=\"optuna\",  # Change to \"random\" for RandomizedSearchCV\n",
    "            scoring=scoring,\n",
    "            cv=cv\n",
    "        ) \n",
    "    else:\n",
    "        results[model_name] = train_and_evaluate_model(\n",
    "            model=config[\"model\"],\n",
    "            model_name=model_name,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_val=X_val,\n",
    "            y_val=y_val,\n",
    "            param_grid=None, # set to None to use Optuna\n",
    "            search_method=\"optuna\",  # Change to \"random\" for RandomizedSearchCV\n",
    "            scoring=scoring,\n",
    "            cv=cv\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_predictions(model, X_val):\n",
    "    \"\"\"Generate predictions and probabilities for evaluation.\"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    return y_pred, y_pred_proba\n",
    "\n",
    "def produce_ROC_PR_curves(y_val, y_pred_proba):\n",
    "    \"\"\"Produce ROC and PR curves along with their metrics.\"\"\"\n",
    "    # ROC\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_val, y_pred_proba)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # PR\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y_val, y_pred_proba)\n",
    "    average_precision = metrics.average_precision_score(y_val, y_pred_proba)\n",
    "    baseline_precision = y_val.sum() / len(y_val)\n",
    "\n",
    "    return (fpr, tpr, roc_auc), (precision, recall, average_precision, baseline_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression (Single feature or basic version)\n",
    "logreg_single = results['Logistic Regression (Single)']['model'].fit(X_train_single, y_train_single)\n",
    "# Logistic Regression (Best Optuna regularization)\n",
    "logreg_multi = results['Logistic Regression']['model'].fit(X_train, y_train)\n",
    "\n",
    "# Random Forest (Optuna-tuned)\n",
    "rf = results['Random Forest']['model'].fit(X_train, y_train)\n",
    "\n",
    "# XGBoost (Optuna-tuned)\n",
    "xgb = results['XGBoost']['model'].fit(X_train, y_train)\n",
    "\n",
    "# XGBoost (selected-tuned)\n",
    "best_xgb = XGBClassifier(\n",
    "                max_depth=6,\n",
    "                learning_rate=0.11483457172157868,\n",
    "                n_estimators=127,\n",
    "                subsample=0.9250144693067383,\n",
    "                colsample_bytree=0.768549938469314,  # You had a formatting issue here; I assumed 0.9\n",
    "                scale_pos_weight=8.893156628847267,\n",
    "                reg_alpha=0.4254910148662459,\n",
    "                reg_lambda=0.5370674064383636,\n",
    "                # use_label_encoder=False,\n",
    "                eval_metric=\"logloss\",\n",
    "                random_state=42\n",
    "            ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce predictions and roc-pr values\n",
    "models_to_evaluate = {\n",
    "    \"Logistic Regression (Single)\": logreg_single,\n",
    "    \"Logistic Regression\": logreg_multi,\n",
    "    \"Random Forest\": rf,\n",
    "    \"XGBoost (Optuna)\": xgb,\n",
    "    \"XGBoost (Selected)\": best_xgb\n",
    "}\n",
    "eval_results = {}\n",
    "for model_name, model in models_to_evaluate.items():\n",
    "    if model_name == \"Logistic Regression (Single)\":\n",
    "        y_pred, y_pred_proba = make_eval_predictions(model, X_val_single)\n",
    "        (fpr, tpr, roc_auc), (precision, recall, avg_prec, base_prec) = produce_ROC_PR_curves(y_val_single, y_pred_proba)\n",
    "    else:\n",
    "        y_pred, y_pred_proba = make_eval_predictions(model, X_val)\n",
    "        (fpr, tpr, roc_auc), (precision, recall, avg_prec, base_prec) = produce_ROC_PR_curves(y_val, y_pred_proba)\n",
    "\n",
    "    eval_results[model_name] = {\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_pred_proba\": y_pred_proba,\n",
    "        \"roc\": (fpr, tpr, roc_auc),\n",
    "        \"pr\": (precision, recall, avg_prec, base_prec)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC - PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Plot side-by-side\n",
    "plt.style.use('seaborn-v0_8-talk')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6), tight_layout=True)\n",
    "\n",
    "### ROC Plot\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Random Guess', color='gray')\n",
    "for model_name, res in eval_results.items():\n",
    "    fpr, tpr, roc_auc = res['roc']\n",
    "    ax1.plot(fpr, tpr, label=f'{model_name}\\n(AUC = {roc_auc:.2f})')\n",
    "# Inline labels\n",
    "# ax1.text(0.5, 0.47, 'Random Guess', color='black', fontsize=10, rotation=43)\n",
    "\n",
    "# Axes and annotations\n",
    "ax1.set_xlim([0.0, 1.05])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve')\n",
    "\n",
    "ax1.text(0.4, 0.6, 'Better', fontsize=14, color='black', fontweight='bold')\n",
    "ax1.text(0.25, 0.55, 'Worse', fontsize=14, color='red', fontweight='bold')\n",
    "ax1.arrow(0.2, 0.6, 0.15, -0.2, head_width=0.02, head_length=0.02, fc='red', ec='red')\n",
    "ax1.arrow(0.45, 0.5, -0.15, 0.2, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "\n",
    "ax1.legend(loc='lower left', bbox_to_anchor=(-0.15, -0.47), ncol=2, fontsize=10)\n",
    "\n",
    "### PR Plot\n",
    "ax2.plot([0, 1], [eval_results[list(eval_results.keys())[0]]['pr'][3]]*2, '--', label='No Skill', color='gray')\n",
    "for model_name, res in eval_results.items():\n",
    "    precision, recall, avg_prec, base_prec = res['pr']\n",
    "    ax2.plot(recall, precision, label=f'{model_name}\\n(AP = {avg_prec:.2f})')\n",
    "\n",
    "# add perfect model lines\n",
    "ax2.axhline(1, color='black', linestyle='--')\n",
    "ax2.axvline(1, color='black', linestyle='--')\n",
    "# Inline labels\n",
    "ax2.annotate(\"Perfect Model\", xy=(0.75, 0.75), xycoords='data', fontsize=12, color='black', rotation=-45)\n",
    "\n",
    "ax2.text(0.4, 0.6, 'Better', fontsize=14, color='black', fontweight='bold')\n",
    "ax2.text(0.53, 0.57, 'Worse', fontsize=14, color='red', fontweight='bold')\n",
    "ax2.arrow(0.55, 0.55, -0.15, -0.2, head_width=0.02, head_length=0.02, fc='red', ec='red')\n",
    "ax2.arrow(0.25, 0.35, 0.15, 0.2, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "\n",
    "# Axes and annotations\n",
    "ax2.set_xlim([0.0, 1.05])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve')\n",
    "\n",
    "ax2.legend(loc='lower left', bbox_to_anchor=(-0.15, -0.47), ncol=2, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of the fraction of relevant items among actually returned items while recall is a measure of the fraction of items that were returned among all items that should have been returned. ‘Relevancy’ here refers to items that are postively labeled, i.e., true positives and false negatives.\n",
    "\n",
    "Precision is defined as the number of true positives over the number of true positives plus the number of false positives.\n",
    "\n",
    "Recall is defined as the number of true posistives over the number of true positives plus the number of false negatives.\n",
    "\n",
    "The PR-curve shows the tradeoff between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision. High precision is achieved by having few false positives in the returned results, and high recall is obtained by having few false negatives in the relevant results. High scores show that the classifier is returning accurate results and a majority of relevant results. \n",
    "\n",
    "A system with high recall but low precision returns most of the relevant items but the proportion of returned results that are incorrectly labeled is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing F1 and Average Precision across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Validation F1 scores\n",
    "lr_single_val_f1 = results['Logistic Regression (Single)']['f1_score']\n",
    "lr_val_f1 = results['Logistic Regression']['f1_score']\n",
    "rf_val_f1 = results['Random Forest']['f1_score']\n",
    "xgb_val_f1 = results['XGBoost']['f1_score']\n",
    "xgb_best_val_f1 = metrics.f1_score(y_val, eval_results[\"XGBoost (Selected)\"][\"y_pred\"])\n",
    "\n",
    "\n",
    "# Validation Average Precision scores\n",
    "lr_single_val_ap = results['Logistic Regression (Single)']['average_precision']\n",
    "lr_val_ap = results['Logistic Regression']['average_precision']\n",
    "rf_val_ap = results['Random Forest']['average_precision']\n",
    "xgb_val_ap = results['XGBoost']['average_precision']\n",
    "xbg_best_ap = metrics.average_precision_score(y_val, eval_results[\"XGBoost (Selected)\"][\"y_pred_proba\"])\n",
    "\n",
    "# offset scores under each label\n",
    "labels = ['Wind Speed\\nThreshold', 'Logistic\\nRegression', 'Random\\nForest', 'XGBoost', 'XGBoost\\n(Selected)']\n",
    "f1_scores = [lr_single_val_f1, lr_val_f1, rf_val_f1, xgb_val_f1, xgb_best_val_f1]\n",
    "ap_scores = [lr_single_val_ap, lr_val_ap, rf_val_ap, xgb_val_ap, xbg_best_ap]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "# bar plot\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.5, zorder=10)\n",
    "rects1 = ax.bar(x - width/2, f1_scores, width, label='F1 Score', color='blue')\n",
    "rects2 = ax.bar(x + width/2, ap_scores, width, label='Average Precision', color='orange')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('F1 and Average Precision Scores by Model for Validation Set')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center', va='top')\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "\n",
    "ax.set_ylim(0,1.05)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Model Results\n",
    "\n",
    "We chose XGBoost. This model will be used.\n",
    "\n",
    "Can I just use the loaded ds to do this? Why do I need to do this individually?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import label, find_objects\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate day of the water year\n",
    "def get_day_of_water_year(date):\n",
    "    # if input is type dt.date convert to dt.datetime\n",
    "    if type(date) == dt.date:\n",
    "        date = dt.datetime(date.year, date.month, date.day)\n",
    "    # if the date is before october 1st, subtract one from the year\n",
    "    if date.month < 10:\n",
    "        year = date.year - 1\n",
    "    else:\n",
    "        year = date.year\n",
    "    # create a datetime object for october 1st of the year\n",
    "    oct_1 = dt.datetime(year, 10, 1)\n",
    "    # calculate the day of water year\n",
    "    day_of_water_year = (date - oct_1).days + 1\n",
    "    return day_of_water_year\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  Build the base prediction DataFrame\n",
    "# ----------------------------------------------------------------------\n",
    "def build_prediction_df(predictions: np.ndarray,\n",
    "                        time_index: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with temporal helper columns already attached.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(predictions, index=time_index,\n",
    "                      columns=['sublimation_event'])\n",
    "\n",
    "    df['day_of_water_year'] = df.index.map(get_day_of_water_year)\n",
    "    df['month'] = df.index.month\n",
    "\n",
    "    # Water‑year: Oct→Sep\n",
    "    df['water_year'] = np.where(df.index.month < 10,\n",
    "                                df.index.year,\n",
    "                                df.index.year + 1)\n",
    "\n",
    "    df['decade'] = (df['water_year'] // 10) * 10\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Detect long events & annotate the DataFrame\n",
    "# ----------------------------------------------------------------------\n",
    "def label_long_events(df: pd.DataFrame,\n",
    "                      col: str = 'sublimation_event',\n",
    "                      min_len: int = 4\n",
    "                      ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds two columns:\n",
    "        • long_event_start : 1 on the first timestep of a long event\n",
    "        • long_event       : 1 on every timestep inside a long event\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame with a binary 'col' column\n",
    "    col : column to search for runs of 1's\n",
    "    min_len : minimum run length (in rows) to be called a 'long' event\n",
    "    \"\"\"\n",
    "    labeled, _ = label(df[col].values == 1)\n",
    "    slices = find_objects(labeled)\n",
    "\n",
    "    df['long_event_start'] = 0\n",
    "    df['long_event'] = 0\n",
    "\n",
    "    for lbl, slc in enumerate(slices, start=1):\n",
    "        if slc is None:\n",
    "            continue\n",
    "        run_len = slc[0].stop - slc[0].start\n",
    "        if run_len >= min_len:\n",
    "            df.iloc[slc[0].start, df.columns.get_loc('long_event_start')] = 1\n",
    "            df.iloc[slc[0], df.columns.get_loc('long_event')] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4.  Summarise discrete long‑event counts\n",
    "# ----------------------------------------------------------------------\n",
    "def count_long_events(df: pd.DataFrame,\n",
    "                      group_cols: Tuple[str, ...] = ('decade', 'water_year', 'month')\n",
    "                      ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a tidy table with one row per group and the number of\n",
    "    discrete long events in `number_of_long_events`.\n",
    "    \"\"\"\n",
    "    out = (\n",
    "        df.groupby(list(group_cols))['long_event_start']\n",
    "          .sum()\n",
    "          .rename('number_of_long_events')\n",
    "          .reset_index()\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Apply to timeseries -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "water_years = np.arange(1980,1982)\n",
    "lat_sel = lat_sel_10_by_10\n",
    "lon_sel = lon_sel_10_by_10\n",
    "cleaned_df_list = []\n",
    "result_df = pd.DataFrame(index=water_years, columns=['no_of_events', 'length_of_winter', 'percentage_of_events', 'sublimation_event_hours','average_wind_speed'])\n",
    "monthly_results = pd.DataFrame(index=water_years, columns=['December', 'January', 'February', 'March'])\n",
    "for water_year in water_years:\n",
    "    print(f\"Opening {water_year} data...\")\n",
    "    winter_slice = slice(f'{water_year-1}-12-01',f'{water_year}-03-31')\n",
    "    # open reanalysis data\n",
    "    tmp_wy1_ds = xr.open_dataset(f'/storage/dlhogan/data/raw_data/ERA5_reanalysis_western_NA_{water_year-1}_v2.nc')\n",
    "    tmp_wy2_ds = xr.open_dataset(f'/storage/dlhogan/data/raw_data/ERA5_reanalysis_western_NA_{water_year}_v2.nc')\n",
    "    print(\"Data opened... Performing temporal subset...\")\n",
    "    # temporal subset\n",
    "    tmp_wy1_subset_ds = subset_ds(tmp_wy1_ds, winter_slice=winter_slice,)\n",
    "    tmp_wy2_subset_ds = subset_ds(tmp_wy2_ds, winter_slice=winter_slice,)\n",
    "    print(\"Temporal subset complete... Performing spatial subset...\")\n",
    "    # spatial subset\n",
    "    tmp_wy1_subset_winter_ds = spatial_subset(tmp_wy1_subset_ds, lat_sel, lon_sel).mean(dim=['latitude', 'longitude'])\n",
    "    tmp_wy2_subset_winter_2023 = spatial_subset(tmp_wy2_subset_ds, lat_sel, lon_sel).mean(dim=['latitude', 'longitude'])\n",
    "    print(\"Spatial subset complete... Merging and cleaning data...\")\n",
    "    combined_df = create_df_combined(tmp_wy1_subset_ds, \n",
    "                                 tmp_wy2_subset_ds, \n",
    "                                 variables, \n",
    "                                 resample=True) \n",
    "    \n",
    "    print(\"Data merged and cleaned...\")\n",
    "    cleaned_df_list[water_year] = combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random binary time series for demonstration with length of 1000\n",
    "np.random.seed(42)  # For reproducibility\n",
    "prediction = np.random.randint(0, 2, size=100000)\n",
    "\n",
    "# example temporal series\n",
    "example_time_series = pd.date_range(start='1980-01-01', periods=len(prediction), freq='3H')\n",
    "\n",
    "# 1) Build the full dataframe once\n",
    "prediction_df = build_prediction_df(prediction, example_time_series)\n",
    "\n",
    "# 2) Mark long events (≥4 consecutive 3‑h steps = ≥12 h here)\n",
    "prediction_df = label_long_events(prediction_df, min_len=4)\n",
    "\n",
    "# 3) Get counts (change group_cols as needed)\n",
    "long_event_counts = count_long_events(\n",
    "    prediction_df,\n",
    "    group_cols=('decade', 'water_year', 'month')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot a timeseries of event hours for each year\n",
    "sns.lineplot(\n",
    "    data=(prediction_df.groupby('water_year')['long_event'].sum()*3).reset_index(),  # multiply by 3 to convert to hours\n",
    "    x=\"water_year\",\n",
    "    y=\"long_event\",\n",
    "    estimator='sum',\n",
    "    marker='o',\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# plot a timeseries of event hours for each year\n",
    "sns.lineplot(\n",
    "    data=(prediction_df.groupby('day_of_water_year')['long_event'].sum()/8).reset_index(), # divide by 8 to get days\n",
    "    x=\"day_of_water_year\",\n",
    "    y=\"long_event\",\n",
    "    estimator='mean',\n",
    "    marker='o',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# plot a boxplot of the long event counts by month in each decade\n",
    "sns.boxplot(\n",
    "    data=long_event_counts,\n",
    "    x=\"month\",\n",
    "    y=\"number_of_long_events\",\n",
    "    hue=\"decade\",\n",
    "    palette=\"tab10\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# plot the the mean distribution of long events across decades for each day of each wateryear\n",
    "mean_long_events = prediction_df.groupby(['decade', 'month'])['long_event'].mean().reset_index()\n",
    "mean_long_events_pivot = mean_long_events.pivot(index='month', columns ='decade', values='long_event')\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=mean_long_events_pivot, dashes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between primary variable and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.scatter(result_df['no_of_events'], result_df['average_wind_speed'])\n",
    "# add the r2 between variables\n",
    "# slope, intercept, r_value, p_value, std_err = linregress(result_df['no_of_events'], result_df['average_wind_speed'])\n",
    "# plt.plot(result_df['no_of_events'], intercept + slope * result_df['no_of_events'], 'r', label='fitted line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary results\n",
    "\n",
    "! Save the data that goes into it so I can produce this figure in my results section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-v0_8-talk')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# set height ratios for subplots\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[2, 2]) \n",
    "\n",
    "# the first subplot\n",
    "ax0 = plt.subplot(gs[0])\n",
    "\n",
    "ax0.plot(result_df.index, result_df['no_of_events']*3, '-o', color='blue', label='Duration of Sublimation Events', markersize=5)\n",
    "ax0.scatter(x=[2022, 2023],\n",
    "            y=[len(w22_large_sublimation_long_events)*3, len(w23_large_sublimation_long_events)*3],\n",
    "            color='red', s=100, label='Observed')\n",
    "ax0.set_ylabel('Duration of Sublimation\\nEvents (hours)')\n",
    "# add horizontal grid\n",
    "ax0.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax0.set_ylim(0,450)\n",
    "\n",
    "# annotate observed values\n",
    "ax0.annotate('Observed', xy=(2022, len(w22_large_sublimation_long_events)*3), xytext=(2018, len(w22_large_sublimation_long_events)*3+75),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->', linewidth=2), fontsize=18, color='black')\n",
    "ax0.annotate('Observed', xy=(2023, len(w23_large_sublimation_long_events)*3), xytext=(2018, len(w22_large_sublimation_long_events)*3+75),\n",
    "                arrowprops=dict(facecolor='black', arrowstyle='->', linewidth=2), fontsize=18, color='black')\n",
    "\n",
    "# annotate the line \n",
    "ax0.annotate('Modeled\\nSublimation\\nEvents', xy=(2024, (result_df['no_of_events']*3).iloc[-1]), \n",
    "             xytext=(2025, (result_df['no_of_events']*3).iloc[-1]), \n",
    "             fontsize=18, \n",
    "             va='center',\n",
    "             color='blue')\n",
    "\n",
    "# remove top and right spine\n",
    "ax0.spines['top'].set_visible(False)\n",
    "ax0.spines['right'].set_visible(False)\n",
    "\n",
    "ax1 = plt.subplot(gs[1], sharex = ax0)  \n",
    "ax1.plot(result_df.index, result_df['average_wind_speed'], '-o', color='darkorange', label='Average Winter 500-mb Wind Speed', markersize=5)\n",
    "ax1.plot(guc_obs.loc[1980:2024].index, guc_obs.loc[1980:2024]['winds'], '-o', color='black', label='Observed GJT Wind Speed', markersize=5)\n",
    "ax1.set_xlabel('Water Year')\n",
    "ax1.set_ylabel('Average Winter\\nWind Speed (m/s)')\n",
    "\n",
    "# annotate the line\n",
    "ax1.annotate('ERA5 500-mb \\nWind Speed', xy=(2024, result_df['average_wind_speed'].iloc[-1]),\n",
    "                xytext=(2025, result_df['average_wind_speed'].iloc[-1]-1), \n",
    "                fontsize=18, \n",
    "                va='center',\n",
    "                color='darkorange')\n",
    "ax1.annotate('GJT 500-mb \\nWind Speed', xy=(2024, result_df['average_wind_speed'].iloc[-2]),\n",
    "                xytext=(2025, result_df['average_wind_speed'].iloc[-2]), \n",
    "                fontsize=18, \n",
    "                va='center',\n",
    "                color='black')\n",
    "\n",
    "# add horizontal grid\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax1.set_ylim(0,23)\n",
    "\n",
    "# remove right spine\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# show every 5 years on the x-axis\n",
    "ax1.set_xticks(np.arange(1980, 2025, 5))\n",
    "ax1.set_xticklabels(np.arange(1980, 2025, 5), fontsize=14)\n",
    "\n",
    "\n",
    "# remove vertical gap between subplots\n",
    "plt.subplots_adjust(hspace=.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,2, tight_layout=True);\n",
    "axs = axs.ravel()\n",
    "cols = ['no_of_events', 'average_wind_speed', 'percentage_of_events', 'sublimation_event_hours']\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    result_df[cols[i]].hist(ax=axs[i], color='blue', bins=8)\n",
    "    axs[i].set_title(cols[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# perform the Mann-Kendall test\n",
    "mk_result_no_events = mk.original_test(result_df['no_of_events'], alpha=0.05)\n",
    "mk_gjt_obs = mk.original_test(guc_obs['winds'], alpha=0.05)\n",
    "mk_result_average_windspeed = mk.original_test(result_df['average_wind_speed'], alpha=0.05)\n",
    "mk_result_percentage_of_events = mk.original_test(result_df['percentage_of_events'], alpha=0.05)\n",
    "\n",
    "print(mk_result_no_events)\n",
    "print(mk_gjt_obs)\n",
    "print(mk_result_average_windspeed)\n",
    "print(mk_result_percentage_of_events)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # plot boxplot of monthly_results\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.boxplot([monthly_results['December']*100, monthly_results['January']*100, monthly_results['February']*100, monthly_results['March']*100],\n",
    "              labels=['December', 'January', 'February', 'March'])\n",
    "ax.set_title('Monthly Sublimation Events Distribution')\n",
    "ax.set_ylabel(\"Monthly sublimation event frequency (%)\")\n",
    "ax.set_xlabel('Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decadal comparison boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example figures...\n",
    "To demonstrate how this cross-validation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1338)\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "n_splits = 5\n",
    "# Generate the class/group data\n",
    "n_points = 100\n",
    "X = rng.randn(100, 10)\n",
    "\n",
    "percentiles_classes = [0.1, 0.9]\n",
    "y = np.hstack([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\n",
    "\n",
    "# Generate uneven groups\n",
    "group_prior = rng.dirichlet([2] * 10)\n",
    "groups = np.repeat(np.arange(10), rng.multinomial(100, group_prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    use_groups = \"Group\" in type(cv).__name__\n",
    "    groups = group if use_groups else None\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=groups)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=cmap_cv,\n",
    "            vmin=-0.2,\n",
    "            vmax=1.2,\n",
    "        )\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    )\n",
    "\n",
    "    # ax.scatter(\n",
    "    #     range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    # )\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n",
    "    ax.set(\n",
    "        yticks=np.arange(n_splits + 2) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "        ylim=[n_splits + 2.2, -0.2],\n",
    "        xlim=[0, 100],\n",
    "    )\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cvs = [StratifiedShuffleSplit, KFold, StratifiedKFold]\n",
    "\n",
    "for cv in cvs:\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    \n",
    "    if cv == StratifiedKFold:\n",
    "        plot_cv_indices(cv(n_splits, shuffle=True, random_state=42), X, y, groups, ax, n_splits)\n",
    "    else:\n",
    "        plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n",
    "    ax.legend(\n",
    "        [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.02))],\n",
    "        [\"Testing set\", \"Training set\"],\n",
    "        loc=(1.02, 0.8),\n",
    "    )\n",
    "    # Make the legend fit\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression (Single variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_single = LogisticRegression(class_weight='balanced', random_state=42, solver='lbfgs', penalty='l2', C=0.5)\n",
    "# GridSearchCV parameters to test\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = np.arange(0.01, 0.5, 0.01)\n",
    "\n",
    "# define the grid\n",
    "param_grid = {\n",
    "    # 'solver': solvers,\n",
    "    # 'penalty': penalty,\n",
    "    'C': c_values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the gridsearch\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lr_single, # our logistic regression model\n",
    "    param_grid=param_grid, # the grid of parameters to test\n",
    "    scoring=scoring, # the scoring metrics to use\n",
    "    refit = 'F1', # the metric to use to select the best model\n",
    "    cv=cv, # the cross-validation object\n",
    "    n_jobs=4, # use 4 cores\n",
    "    return_train_score=True, # return the training score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model\n",
    "results_lr = grid_search.fit(X_train_single, y_train_single)\n",
    "results_lr_cv = results_lr.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\n",
    "\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(min(results_lr_cv[f\"param_{list(param_grid.keys())[0]}\"]), max(results_lr_cv[f\"param_{list(param_grid.keys())[0]}\"]))\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results_lr_cv[f\"param_{list(param_grid.keys())[0]}\"].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(sorted(scoring), [\"g\", \"k\"]):\n",
    "    for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "        sample_score_mean = results_lr_cv[\"mean_%s_%s\" % (sample, scorer)]\n",
    "        sample_score_std = results_lr_cv[\"std_%s_%s\" % (sample, scorer)]\n",
    "        ax.fill_between(\n",
    "            X_axis,\n",
    "            sample_score_mean - sample_score_std,\n",
    "            sample_score_mean + sample_score_std,\n",
    "            alpha=0.1 if sample == \"test\" else 0,\n",
    "            color=color,\n",
    "        )\n",
    "        ax.plot(\n",
    "            X_axis,\n",
    "            sample_score_mean,\n",
    "            style,\n",
    "            color=color,\n",
    "            alpha=1 if sample == \"test\" else 0.7,\n",
    "            label=\"%s (%s)\" % (scorer, sample),\n",
    "        )\n",
    "\n",
    "    best_index = np.nonzero(results_lr_cv[\"rank_test_%s\" % scorer] == 1)[0][0]\n",
    "    best_score = results_lr_cv[\"mean_test_%s\" % scorer][best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot(\n",
    "        [\n",
    "            X_axis[best_index],\n",
    "        ]\n",
    "        * 2,\n",
    "        [0, best_score],\n",
    "        linestyle=\"-.\",\n",
    "        color=color,\n",
    "        marker=\"x\",\n",
    "        markeredgewidth=3,\n",
    "        ms=8,\n",
    "    )\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score, (X_axis[best_index], best_score + 0.005))\n",
    "    # Annotate the best parameter for that scorer\n",
    "    ax.annotate(\n",
    "        \"%s\" % round(X_axis[best_index],1),\n",
    "        (X_axis[best_index], -0.01),\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (results_lr.best_score_, results_lr.best_params_))\n",
    "means = results_lr_cv['mean_test_F1']\n",
    "stds = results_lr_cv['std_test_F1']\n",
    "params = results_lr_cv['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the best model and classify results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the model\n",
    "lr_single_best = LogisticRegression(class_weight='balanced', \n",
    "                             random_state=42, \n",
    "                             solver='lbfgs', \n",
    "                             penalty='l2', \n",
    "                             C=0.03).fit(X_train_single, y_train_single)\n",
    "lr_single_y_pred_proba = lr_single_best.predict_proba(X_val_single)[:, 1]\n",
    "\n",
    "# ROC\n",
    "lr_single_fpr, lr_single_tpr, _ = metrics.roc_curve(y_val_single, lr_single_y_pred_proba)\n",
    "lr_single_roc_auc = metrics.auc(lr_single_fpr, lr_single_tpr)\n",
    "\n",
    "# PR\n",
    "lr_single_precision, lr_single_recall, _ = metrics.precision_recall_curve(y_val_single, lr_single_y_pred_proba)\n",
    "lr_single_average_precision = metrics.average_precision_score(y_val_single, lr_single_y_pred_proba)\n",
    "lr_single_baseline_precision = y_val_single.sum() / len(y_val_single)\n",
    "\n",
    "# show confusion matrix\n",
    "metrics.ConfusionMatrixDisplay.from_estimator(lr_single_best, X_val_single, y_val_single, cmap='Blues', colorbar=False, display_labels=['non-event', 'event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Multi-variable Logistic Regressor\n",
    "\n",
    "Some background. Logistic \"regression\" is a classification algorithm that is used to predict the *probability* of a categorical dependent value. It sounds similar to linear regression in some ways, but it is distinctly different because the dependent and independent variables are not related linearly, instead the linear relationship is between the independent variables and the logit (log of odds) of independent variable, essentially its probability. The method can then be used to predict the probability of occurrence of a binary event utilizing a logit function (like Sigmoid/logistic function)\n",
    "$$p = \\frac{1}{1+e^{-y}}$$\n",
    "\n",
    "where $y = \\Beta_0 +\\Beta_1X_1 +\\Beta_2X_2 ...+\\Beta_nX_n$\n",
    "\n",
    "Logistic regression uses maximum liklihood estimation approach. MLE is a liklihood maximation method while OLS is a distance-minimizing method. Maximizing the liklihood function determines parameters that are most likely to produce the observed data\n",
    "\n",
    "Model choices include:\n",
    "- not standardized (insensitive to magnitude)\n",
    "- 80/20 train test split for validation\n",
    "- cross validation on k=10 folds with stratified shuffle split.\n",
    "- performance is evaluated using f1-score, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced', random_state=42, solver='lbfgs', penalty='l2', C=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV parameters to test\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = np.arange(0.01, 0.5, 0.01)\n",
    "\n",
    "# define the grid\n",
    "param_grid = {\n",
    "    # 'solver': solvers,\n",
    "    # 'penalty': penalty,\n",
    "    'C': c_values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the gridsearch\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lr, # our logistic regression model\n",
    "    param_grid=param_grid, # the grid of parameters to test\n",
    "    scoring=scoring, # the scoring metrics to use\n",
    "    refit = 'F1', # the metric to use to select the best model\n",
    "    cv=cv, # the cross-validation object\n",
    "    n_jobs=4, # use 4 cores\n",
    "    return_train_score=True, # return the training score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# run the model\n",
    "results_lr = grid_search.fit(X_train, y_train)\n",
    "results_lr_cv = results_lr.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\n",
    "\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(min(results_lr_cv[f\"param_{list(param_grid.keys())[0]}\"]), max(results_lr_cv[f\"param_{list(param_grid.keys())[0]}\"]))\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results_lr_cv[f\"param_{list(param_grid.keys())[0]}\"].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(sorted(scoring), [\"g\", \"k\"]):\n",
    "    for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "        sample_score_mean = results_lr_cv[\"mean_%s_%s\" % (sample, scorer)]\n",
    "        sample_score_std = results_lr_cv[\"std_%s_%s\" % (sample, scorer)]\n",
    "        ax.fill_between(\n",
    "            X_axis,\n",
    "            sample_score_mean - sample_score_std,\n",
    "            sample_score_mean + sample_score_std,\n",
    "            alpha=0.1 if sample == \"test\" else 0,\n",
    "            color=color,\n",
    "        )\n",
    "        ax.plot(\n",
    "            X_axis,\n",
    "            sample_score_mean,\n",
    "            style,\n",
    "            color=color,\n",
    "            alpha=1 if sample == \"test\" else 0.7,\n",
    "            label=\"%s (%s)\" % (scorer, sample),\n",
    "        )\n",
    "\n",
    "    best_index = np.nonzero(results_lr_cv[\"rank_test_%s\" % scorer] == 1)[0][0]\n",
    "    best_score = results_lr_cv[\"mean_test_%s\" % scorer][best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot(\n",
    "        [\n",
    "            X_axis[best_index],\n",
    "        ]\n",
    "        * 2,\n",
    "        [0, best_score],\n",
    "        linestyle=\"-.\",\n",
    "        color=color,\n",
    "        marker=\"x\",\n",
    "        markeredgewidth=3,\n",
    "        ms=8,\n",
    "    )\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score, (X_axis[best_index], best_score + 0.005))\n",
    "    # Annotate the best parameter for that scorer\n",
    "    ax.annotate(\n",
    "        \"%s\" % round(X_axis[best_index],1),\n",
    "        (X_axis[best_index], -0.01),\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (results_lr.best_score_, results_lr.best_params_))\n",
    "means = results_lr_cv['mean_test_F1']\n",
    "stds = results_lr_cv['std_test_F1']\n",
    "params = results_lr_cv['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the best model and classify results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the model\n",
    "lr_best = LogisticRegression(class_weight='balanced', \n",
    "                             random_state=42, \n",
    "                             solver='lbfgs', \n",
    "                             penalty='l2', \n",
    "                             C=0.03).fit(X_train, y_train)\n",
    "lr_y_pred_proba = lr_best.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# ROC\n",
    "lr_fpr, lr_tpr, _ = metrics.roc_curve(y_val, lr_y_pred_proba)\n",
    "lr_roc_auc = metrics.auc(lr_fpr, lr_tpr)\n",
    "\n",
    "# PR\n",
    "lr_precision, lr_recall, _ = metrics.precision_recall_curve(y_val, lr_y_pred_proba)\n",
    "lr_average_precision = metrics.average_precision_score(y_val, lr_y_pred_proba)\n",
    "lr_baseline_precision = y_val.sum() / len(y_val)\n",
    "\n",
    "# show confusion matrix\n",
    "metrics.ConfusionMatrixDisplay.from_estimator(lr_best, X_val, y_val, cmap='Blues', colorbar=False, display_labels=['non-event', 'event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Type             | max_depth | max_features | n_estimators | min_samples_split | min_samples_leaf | Mean F1-Score |\n",
    "|------------------------|-----------|---------------|---------------|--------------------|-------------------|----------------|\n",
    "| Areal Average          | 7         | 3             | 20            | 10                 | —                 | 0.500          |\n",
    "| Closest Point          | 10        | 4             | 60            | 5                  | 4                 | 0.520          |\n",
    "| 5-Grid Areal Average   | 10        | 4             | —             | 2                  | 1                 | 0.545 (±0.109) |\n",
    "| 17×17 Grid             | 15        | 4             | 5             | 2                  | 2                 | 0.589          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, \n",
    "                            max_depth=15, \n",
    "                            max_features=4,\n",
    "                            n_estimators=30, \n",
    "                            min_samples_split=2,\n",
    "                            min_samples_leaf=2,\n",
    "                            )\n",
    "run_description=input(\"Enter a description for the run: \")\n",
    "# define the grid\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 20, 30, 40, 50,100], # number of trees in the forest\n",
    "    # 'max_depth': [5,7,10,15], # maximum depth of the tree, balances overfitting and underfitting\n",
    "    # 'min_samples_split': [2, 5, 10,15,20], # minimum number of samples required to split an internal node\n",
    "    # 'min_samples_leaf': [1, 2, 4, 5, 10, 15, 20], # minimum number of samples required to be at a leaf node\n",
    "    # 'max_features': [2,3,4,5], # number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# scoring metrics\n",
    "scoring = {\"F1\":\"f1\", \n",
    "           \"Average_Precision\":metrics.make_scorer(metrics.average_precision_score),\n",
    "        #    \"Recall\":metrics.make_scorer(metrics.recall_score),\n",
    "        #    \"Precision\":metrics.make_scorer(metrics.precision_score),\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf, # our logistic regression model\n",
    "    param_grid=param_grid, # the grid of parameters to test\n",
    "    scoring=scoring, # the scoring metrics to use\n",
    "    refit = 'F1', # the metric to use to select the best model\n",
    "    cv=cv, # the cross-validation object\n",
    "    n_jobs=4, # use 4 cores\n",
    "    return_train_score=True, # return the training score\n",
    ")\n",
    "results_rf = rf_grid_search.fit(X_train, y_train)\n",
    "results_rf_cv = results_rf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\n",
    "\n",
    "plt.xlabel(list(param_grid.keys())[0])\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(min(results_rf_cv[f\"param_{list(param_grid.keys())[0]}\"]), max(results_rf_cv[f\"param_{list(param_grid.keys())[0]}\"]))\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results_rf_cv[f\"param_{list(param_grid.keys())[0]}\"].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(sorted(scoring), [\"g\", \"k\", ]):\n",
    "    for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "        sample_score_mean = results_rf_cv[\"mean_%s_%s\" % (sample, scorer)]\n",
    "        sample_score_std = results_rf_cv[\"std_%s_%s\" % (sample, scorer)]\n",
    "        ax.fill_between(\n",
    "            X_axis,\n",
    "            sample_score_mean - sample_score_std,\n",
    "            sample_score_mean + sample_score_std,\n",
    "            alpha=0.1 if sample == \"test\" else 0,\n",
    "            color=color,\n",
    "        )\n",
    "        ax.plot(\n",
    "            X_axis,\n",
    "            sample_score_mean,\n",
    "            style,\n",
    "            color=color,\n",
    "            alpha=1 if sample == \"test\" else 0.7,\n",
    "            label=\"%s (%s)\" % (scorer, sample),\n",
    "        )\n",
    "\n",
    "    best_index = np.nonzero(results_rf_cv[\"rank_test_%s\" % scorer] == 1)[0][0]\n",
    "    best_score = results_rf_cv[\"mean_test_%s\" % scorer][best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot(\n",
    "        [\n",
    "            X_axis[best_index],\n",
    "        ]\n",
    "        * 2,\n",
    "        [0, best_score],\n",
    "        linestyle=\"-.\",\n",
    "        color=color,\n",
    "        marker=\"x\",\n",
    "        markeredgewidth=3,\n",
    "        ms=8,\n",
    "    )\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score, (X_axis[best_index], best_score + 0.005))\n",
    "    # Annotate the best parameter for that scorer\n",
    "    ax.annotate(\n",
    "        \"%s\" % round(X_axis[best_index],1),\n",
    "        (X_axis[best_index], -0.01),\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (results_rf.best_score_, results_rf.best_params_))\n",
    "means = results_rf_cv['mean_test_F1']\n",
    "stds = results_rf_cv['std_test_F1']\n",
    "params = results_rf_cv['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_rf_model_run(\n",
    "    run_description,\n",
    "    model_type,\n",
    "    f1_score,\n",
    "    n_estimators=None,\n",
    "    max_depth=None,\n",
    "    min_samples_split=None,\n",
    "    min_samples_leaf=None,\n",
    "    max_features=None,\n",
    "    output_csv=\"rf_model_run_log.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs the summary of a Random Forest model run to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        run_description (str): Brief explanation of the uniqueness of this model run.\n",
    "        model_type (str): The general type or configuration name of the model (e.g., 'areal average', 'closest point').\n",
    "        f1_score (float): The mean F1-score for this model run.\n",
    "        All other parameters are optional hyperparameters used in the run.\n",
    "        output_csv (str): Path to the output CSV file (default: \"rf_model_run_log.csv\").\n",
    "    \"\"\"\n",
    "\n",
    "    row = pd.DataFrame([{\n",
    "        \"run_description\": run_description,\n",
    "        \"model_type\": model_type,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"min_samples_split\": min_samples_split,\n",
    "        \"min_samples_leaf\": min_samples_leaf,\n",
    "        \"max_features\": max_features\n",
    "    }])\n",
    "\n",
    "    if os.path.exists(output_csv):\n",
    "        row.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        row.to_csv(output_csv, mode='w', header=True, index=False)\n",
    "\n",
    "    print(f\"✅ Random Forest model run logged successfully to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "best_f1_score = results_rf.best_score_\n",
    "best_params = results_rf.best_params_\n",
    "\n",
    "if 'n_estimators' not in best_params.keys():\n",
    "    best_params['n_estimators'] = rf.n_estimators\n",
    "if 'max_depth' not in best_params.keys():\n",
    "    best_params['max_depth'] = rf.max_depth\n",
    "if 'min_samples_split' not in best_params.keys():\n",
    "    best_params['min_samples_split'] = rf.min_samples_split\n",
    "if 'min_samples_leaf' not in best_params.keys():\n",
    "    best_params['min_samples_leaf'] = rf.min_samples_leaf\n",
    "if 'max_features' not in best_params.keys():\n",
    "    best_params['max_features'] = rf.max_features\n",
    "\n",
    "log_rf_model_run(\n",
    "    run_description=run_description,\n",
    "    model_type=\"Areal Average\",\n",
    "    f1_score=best_f1_score,\n",
    "    n_estimators=best_params.get('n_estimators'),\n",
    "    max_depth=best_params.get('max_depth'),\n",
    "    min_samples_split=best_params.get('min_samples_split'),\n",
    "    min_samples_leaf=best_params.get('min_samples_leaf'),\n",
    "    max_features=best_params.get('max_features')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "important_features = pd.Series(data=results_rf.best_estimator_.feature_importances_,index=data_X.columns)\n",
    "important_features.sort_values(ascending=False,inplace=True)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "important_features.plot(kind='bar')\n",
    "plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the model\n",
    "rf_best = RandomForestClassifier(random_state=42, \n",
    "                            n_estimators=best_params.get('n_estimators'),\n",
    "                            max_depth=best_params.get('max_depth'),\n",
    "                            min_samples_split=best_params.get('min_samples_split'),\n",
    "                            min_samples_leaf=best_params.get('min_samples_leaf'),\n",
    "                            max_features=best_params.get('max_features')\n",
    "                            ).fit(X_train, y_train)\n",
    "                                 \n",
    "rf_y_pred_proba = rf_best.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# ROC\n",
    "rf_fpr, rf_tpr, _ = metrics.roc_curve(y_val, rf_y_pred_proba)\n",
    "rf_roc_auc = metrics.auc(rf_fpr, rf_tpr)\n",
    "\n",
    "# PR\n",
    "rf_precision, rf_recall, _ = metrics.precision_recall_curve(y_val, rf_y_pred_proba)\n",
    "rf_average_precision = metrics.average_precision_score(y_val, rf_y_pred_proba)\n",
    "rf_baseline_precision = y_val.sum() / len(y_val)\n",
    "\n",
    "# Show confusion matrix\n",
    "metrics.ConfusionMatrixDisplay.from_estimator(rf_best, X_val, y_val, cmap='Blues', colorbar=False, display_labels=['non-event', 'event'])\n",
    "# print f1 score\n",
    "print(\"F1 Score: \", metrics.f1_score(y_val, rf_best.predict(X_val)))\n",
    "# print average precision score\n",
    "print(f\"AP: {metrics.average_precision_score(y_val, rf_best.predict(X_val))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: XGBoost Classifier\n",
    "\n",
    "| Model Type           | n_estimators | learning_rate | max_depth | scale_pos_weight | subsample | Mean F1-Score |\n",
    "|----------------------|--------------|----------------|-----------|-------------------|------------|----------------|\n",
    "| Areal Average        | 30           | 0.3            | 7         | 10                | 0.3        | 0.640          |\n",
    "| Closest Point        | —            | 0.7            | 7         | 5                 | 0.7        | 0.571          |\n",
    "| 5-Grid Average       | 30           | 0.2            | 10        | 8                 | 0.5        | 0.590          |\n",
    "| 10-Grid Average      | 60           | 0.1            | 10        | 5                 | 0.7        | 0.610          |\n",
    "| 20-Grid Average      | —            | 0.2            | 7         | 10                | 0.5        | 0.660          |\n",
    "| 17×17 Grid           | 80           | 0.5            | 7         | 50                | 0.9        | 0.630          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=42, \n",
    "                                   n_estimators=30, \n",
    "                                   scale_pos_weight=20, \n",
    "                                   learning_rate=0.3,\n",
    "                                   max_depth=7,\n",
    "                                   subsample= 0.9,\n",
    "                                   objective=\"binary:logistic\"\n",
    "                                )\n",
    "# Add a short description of what makes this run unique\n",
    "run_description = input(\"Enter a brief description of this XGBoost model run: \")\n",
    "model_area = input(\"Enter the model area (e.g., '10-grid Areal Average', 'Closest Point'): \")\n",
    "\n",
    "default = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "# Define the range of scale_pos_weight ratios to test\n",
    "ratios = [5, 10, 20, 50, 100]\n",
    "# define the grid\n",
    "param_grid = {\n",
    "    'learning_rate':  [0.1, 0.2, 0.3,0.5,0.7], # st controls the step size at each boosting iteration\n",
    "   # 'n_estimators': np.arange(10,50,10), # number of trees in the forest\n",
    "   #  'max_depth': [2,3,5,7,10], # controls the maximum depth of the decision trees used in the model\n",
    "    'scale_pos_weight': ratios, # controls the balance of positive and negative weights\n",
    "    'subsample': [0.1,0.3, 0.5, 0.7, 0.9, 1.], # fraction of samples to be used for each tree\n",
    "}\n",
    "\n",
    "# scoring metrics\n",
    "scoring = {\"F1\":\"f1\", \n",
    "           \"Average_Precision\":metrics.make_scorer(metrics.average_precision_score),\n",
    "        #    \"Recall\":metrics.make_scorer(metrics.recall_score),\n",
    "        #    \"Precision\":metrics.make_scorer(metrics.precision_score),\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb, # our logistic regression model\n",
    "    param_grid=param_grid, # the grid of parameters to test\n",
    "    scoring=scoring, # the scoring metrics to use\n",
    "    refit = 'F1', # the metric to use to select the best model\n",
    "    cv=cv, # the cross-validation object\n",
    "    n_jobs=4, # use 4 cores\n",
    "    return_train_score=True, # return the training score\n",
    ")\n",
    "results_xgb = xgb_grid_search.fit(X_train, y_train)\n",
    "results_xgb_cv = results_xgb.cv_results_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_run(\n",
    "    run_description,\n",
    "    model_type,\n",
    "    f1_score,\n",
    "    n_estimators=None,\n",
    "    learning_rate=None,\n",
    "    max_depth=None,\n",
    "    scale_pos_weight=None,\n",
    "    subsample=None,\n",
    "    min_samples_split=None,\n",
    "    min_samples_leaf=None,\n",
    "    max_features=None,\n",
    "    output_csv=\"model_run_log.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs the summary of a model run to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        run_description (str): Brief explanation of the uniqueness of this model run.\n",
    "        model_type (str): The general type or configuration name of the model (e.g., 'areal average', 'closest point').\n",
    "        f1_score (float): The mean F1-score for this model run.\n",
    "        All other parameters are optional hyperparameters used in the run.\n",
    "        output_csv (str): Path to the output CSV file (default: \"model_run_log.csv\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Build a single-row DataFrame\n",
    "    row = pd.DataFrame([{\n",
    "        \"run_description\": run_description,\n",
    "        \"model_type\": model_type,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "        \"subsample\": subsample,\n",
    "        \"min_samples_split\": min_samples_split,\n",
    "        \"min_samples_leaf\": min_samples_leaf,\n",
    "        \"max_features\": max_features\n",
    "    }])\n",
    "\n",
    "    # Append to CSV, creating file if it doesn't exist\n",
    "    if os.path.exists(output_csv):\n",
    "        row.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        row.to_csv(output_csv, mode='w', header=True, index=False)\n",
    "\n",
    "    print(f\"✅ Model run logged successfully: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Assuming you've already run a GridSearchCV or RandomizedSearchCV:\n",
    "best_f1_score = results_xgb.best_score_\n",
    "best_params = results_xgb.best_params_\n",
    "\n",
    "if 'n_estimators' not in best_params.keys():\n",
    "    best_params['n_estimators'] = xgb.n_estimators\n",
    "if 'learning_rate' not in best_params.keys():\n",
    "    best_params['learning_rate'] = xgb.learning_rate\n",
    "if 'max_depth' not in best_params.keys():\n",
    "    best_params['max_depth'] = xgb.max_depth\n",
    "if 'scale_pos_weight' not in best_params.keys():\n",
    "    best_params['scale_pos_weight'] = xgb.scale_pos_weight\n",
    "if 'subsample' not in best_params.keys():\n",
    "    best_params['subsample'] = xgb.subsample\n",
    "if 'max_features' not in best_params.keys():\n",
    "    best_params['max_features'] = xgb.max_features if hasattr(xgb, 'max_features') else None  # optional; only include if used\n",
    "\n",
    "# Now call the log function\n",
    "log_model_run(\n",
    "    run_description=run_description,\n",
    "    model_type=model_area,\n",
    "    f1_score=best_f1_score,\n",
    "    n_estimators=best_params.get('n_estimators'),\n",
    "    learning_rate=best_params.get('learning_rate'),\n",
    "    max_depth=best_params.get('max_depth'),\n",
    "    scale_pos_weight=best_params.get('scale_pos_weight'),\n",
    "    subsample=best_params.get('subsample'),\n",
    "    max_features=best_params.get('max_features')  # optional; only include if used\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (results_xgb.best_score_, results_xgb.best_params_))\n",
    "means = results_xgb_cv['mean_test_F1']\n",
    "stds = results_xgb_cv['std_test_F1']\n",
    "params = results_xgb_cv['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(param_grid.keys()) == 1:\n",
    "    # Plot the results for a single parameter grid\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\n",
    "\n",
    "    plt.xlabel(list(param_grid.keys())[0])\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(min(results_xgb_cv[f\"param_{list(param_grid.keys())[0]}\"]), max(results_xgb_cv[f\"param_{list(param_grid.keys())[0]}\"]))\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results_xgb_cv[f\"param_{list(param_grid.keys())[0]}\"].data, dtype=float)\n",
    "\n",
    "    for scorer, color in zip(sorted(scoring), [\"g\", \"k\", ]):\n",
    "        for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "            sample_score_mean = results_xgb_cv[\"mean_%s_%s\" % (sample, scorer)]\n",
    "            sample_score_std = results_xgb_cv[\"std_%s_%s\" % (sample, scorer)]\n",
    "            ax.fill_between(\n",
    "                X_axis,\n",
    "                sample_score_mean - sample_score_std,\n",
    "                sample_score_mean + sample_score_std,\n",
    "                alpha=0.1 if sample == \"test\" else 0,\n",
    "                color=color,\n",
    "            )\n",
    "            ax.plot(\n",
    "                X_axis,\n",
    "                sample_score_mean,\n",
    "                style,\n",
    "                color=color,\n",
    "                alpha=1 if sample == \"test\" else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample),\n",
    "            )\n",
    "\n",
    "        best_index = np.nonzero(results_xgb_cv[\"rank_test_%s\" % scorer] == 1)[0][0]\n",
    "        best_score = results_xgb_cv[\"mean_test_%s\" % scorer][best_index]\n",
    "\n",
    "        # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "        ax.plot(\n",
    "            [\n",
    "                X_axis[best_index],\n",
    "            ]\n",
    "            * 2,\n",
    "            [0, best_score],\n",
    "            linestyle=\"-.\",\n",
    "            color=color,\n",
    "            marker=\"x\",\n",
    "            markeredgewidth=3,\n",
    "            ms=8,\n",
    "        )\n",
    "\n",
    "        # Annotate the best score for that scorer\n",
    "        ax.annotate(\"%0.2f\" % best_score, (X_axis[best_index], best_score + 0.005))\n",
    "        # Annotate the best parameter for that scorer\n",
    "        ax.annotate(\n",
    "            \"%s\" % round(X_axis[best_index],1),\n",
    "            (X_axis[best_index], 0.1),\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "        )\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "important_features = pd.Series(data=results_xgb.best_estimator_.feature_importances_,index=data_X.columns)\n",
    "important_features.sort_values(ascending=False,inplace=True)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "important_features.plot(kind='bar')\n",
    "plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the best model and classify results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = XGBClassifier(random_state=42, \n",
    "               n_estimators=best_params.get('n_estimators'),\n",
    "               learning_rate=best_params.get('learning_rate'),\n",
    "               max_depth=best_params.get('max_depth'),\n",
    "               scale_pos_weight=best_params.get('scale_pos_weight'),\n",
    "               subsample=best_params.get('subsample'),\n",
    "               objective=\"binary:logistic\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "\n",
    "y_pred_proba = xgb_best.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# ROC\n",
    "xgb_fpr, xgb_tpr, _ = metrics.roc_curve(y_val, y_pred_proba)\n",
    "xgb_roc_auc = metrics.auc(xgb_fpr, xgb_tpr)\n",
    "\n",
    "# PR\n",
    "xgb_precision, xgb_recall, _ = metrics.precision_recall_curve(y_val, y_pred_proba)\n",
    "xgb_average_precision = metrics.average_precision_score(y_val, y_pred_proba)\n",
    "xgb_baseline_precision = y_val.sum() / len(y_val)\n",
    "\n",
    "# # Show confusion matrix\n",
    "# metrics.ConfusionMatrixDisplay.from_estimator(xgb_best, X_val, y_val, cmap='Blues', colorbar=False, display_labels=['non-event', 'event'])\n",
    "# # print f1 score\n",
    "# print(\"F1 Score: \", metrics.f1_score(y_val, xgb_best.predict(X_val)))\n",
    "# # print average precision score\n",
    "# print(f\"AP: {metrics.average_precision_score(y_val, xgb_best.predict(X_val))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot with plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import cufflinks as cf\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "# make plotly work \n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=result_df.index,\n",
    "    y=result_df['no_of_events']*3,\n",
    "    mode='lines+markers',\n",
    "    name='Percentage of Events',\n",
    "    line=dict(color='blue', width=2),   \n",
    "), secondary_y=False,)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[2022, 2023],\n",
    "    y=[len(w22_large_sublimation_long_events)*3, len(w23_large_sublimation_long_events)*3],\n",
    "    mode='markers',\n",
    "    name='Observed',\n",
    "    marker=dict(color='red', size=10)\n",
    "), secondary_y=False,)  \n",
    "\n",
    "# add secondary axis with average wind speed\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=result_df.index,\n",
    "    y=result_df['average_wind_speed'],\n",
    "    mode='lines+markers',\n",
    "    name='Average Wind Speed',\n",
    "    line=dict(color='orange', width=2),\\\n",
    "), secondary_y=True,)\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=400,\n",
    "    title='Total Number of Hours with Sublimation Events by Winter',\n",
    "    xaxis_title='Water Year',\n",
    "    yaxis_title='Hours',\n",
    "    yaxis2_title='Average Wind Speed (m/s)',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sublime_synoptics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
